{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TCC"
      ],
      "metadata": {
        "id": "UOkSSqYax-Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 1: Coleta e Validação dos Dados Brutos"
      ],
      "metadata": {
        "id": "9QdD9wiKyneG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Introdução\n",
        "\n"
      ],
      "metadata": {
        "id": "KjHbnHuMyryD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O ponto de partida de qualquer projeto de *data science* é a obtenção e validação dos dados. Esta etapa inicial é fundamental para garantir a integridade, a completude e a consistência estrutural dos dados brutos que servirão de alicerce para todas as fases subsequentes de análise e modelagem."
      ],
      "metadata": {
        "id": "XDu9jGy5ytaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Objetivos\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uKPlI96gyXXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os objetivos desta etapa são:\n",
        "1.  **Estabelecer os caminhos** para os diretórios de entrada (dados brutos) e saída (logs e artefatos da Etapa 1).\n",
        "2.  **Validar a presença** de todos os arquivos anuais esperados (2006 a 2023) no formato `.xlsx`.\n",
        "3.  **Verificar a estrutura interna** de cada arquivo, assegurando que todas as planilhas mensais (`JAN`, `FEV`, ..., `DEZ`) existem.\n",
        "4.  **Garantir a consistência do esquema**, confirmando que todas as planilhas, em todos os arquivos, possuem o mesmo conjunto de colunas.\n",
        "5.  **Gerar um log detalhado** de todo o processo de validação, registrando sucessos, avisos e erros para futura referência."
      ],
      "metadata": {
        "id": "CKVakRVvyynE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Metodologia\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVVophhgy08-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A validação será executada por meio de um conjunto de funções programáticas que irão inspecionar sistematicamente os arquivos e suas estruturas. O resultado de cada verificação será impresso no console e armazenado em um arquivo de log, localizado em um diretório específico para esta etapa, garantindo que o processo seja auditável e reprodutível."
      ],
      "metadata": {
        "id": "I5QM3YxAy2hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "RAW_DATA_DIR = os.path.join(DRIVE_BASE_PATH, 'xlsx_tratados')\n",
        "\n",
        "STAGE_01_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_01_validation')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_01_OUTPUT_DIR, 'log_etapa_1.txt')\n",
        "\n",
        "os.makedirs(STAGE_01_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE VALIDAÇÃO DA ETAPA 1 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*40 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 1 criado em: {STAGE_01_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZMS0o1EyZhM",
        "outputId": "fa34b8c1-9d9d-4ea4-8642-ffcffaebd67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório de saída da Etapa 1 criado em: /content/drive/MyDrive/dados_tcc/stage_01_validation\n",
            "Arquivo de log inicializado em: /content/drive/MyDrive/dados_tcc/stage_01_validation/log_etapa_1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4. Funções de Validação\n"
      ],
      "metadata": {
        "id": "MoOHk5epyb8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para manter o código organizado e reutilizável, a lógica de validação foi encapsulada em funções específicas. Cada função tem uma responsabilidade única, facilitando a manutenção e a depuração.\n",
        "\n",
        "-   **`log_mensagem`**: Uma função auxiliar para registrar mensagens tanto no console quanto no arquivo de log.\n",
        "-   **`validar_anos`**: Verifica a existência dos arquivos `.xlsx` para cada ano esperado.\n",
        "-   **`validar_meses_em_sheets`**: Confere se cada arquivo anual contém as 12 planilhas mensais.\n",
        "-   **`validar_esquema_colunas`**: Compara as colunas de todas as planilhas com um esquema de referência para garantir a consistência."
      ],
      "metadata": {
        "id": "CyrBtb4ny7tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"\n",
        "    Registra uma mensagem formatada no console e no arquivo de log.\n",
        "\n",
        "    Args:\n",
        "        mensagem (str): A mensagem a ser registrada.\n",
        "        tipo (str): O tipo de mensagem (ex: 'INFO', 'AVISO', 'ERRO').\n",
        "    \"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def validar_anos(diretorio, anos_esperados):\n",
        "    \"\"\"\n",
        "    Valida se os arquivos XLSX para os anos esperados existem no diretório.\n",
        "\n",
        "    Args:\n",
        "        diretorio (str): Caminho para a pasta com os arquivos XLSX.\n",
        "        anos_esperados (list): Lista de inteiros representando os anos.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Uma tupla contendo (lista de anos encontrados, lista de anos faltantes).\n",
        "    \"\"\"\n",
        "    log_mensagem(\"Iniciando validação de anos existentes...\")\n",
        "    try:\n",
        "        anos_existentes = [int(nome.split('.')[0]) for nome in os.listdir(diretorio) if nome.endswith('.xlsx')]\n",
        "        anos_faltantes = [ano for ano in anos_esperados if ano not in anos_existentes]\n",
        "\n",
        "        if not anos_faltantes:\n",
        "            log_mensagem(\"Todos os anos esperados foram encontrados.\")\n",
        "        else:\n",
        "            log_mensagem(f\"Anos faltantes: {anos_faltantes}\", tipo='AVISO')\n",
        "\n",
        "        log_mensagem(f\"Anos encontrados: {sorted(anos_existentes)}\")\n",
        "        return sorted(anos_existentes), anos_faltantes\n",
        "    except Exception as e:\n",
        "        log_mensagem(f\"Falha ao validar anos: {e}\", tipo='ERRO')\n",
        "        return [], anos_esperados\n",
        "\n",
        "def validar_meses_em_sheets(diretorio, anos_a_verificar):\n",
        "    \"\"\"\n",
        "    Verifica se cada arquivo anual contém as 12 planilhas mensais.\n",
        "\n",
        "    Args:\n",
        "        diretorio (str): Caminho para a pasta com os arquivos XLSX.\n",
        "        anos_a_verificar (list): Lista de anos cujos arquivos serão verificados.\n",
        "\n",
        "    Returns:\n",
        "        dict: Um dicionário com anos como chave e listas de sheets faltantes como valor.\n",
        "    \"\"\"\n",
        "    log_mensagem(\"\\nIniciando validação de sheets mensais...\")\n",
        "    meses_esperados = ['JAN', 'FEV', 'MAR', 'ABR', 'MAI', 'JUN', 'JUL', 'AGO', 'SET', 'OUT', 'NOV', 'DEZ']\n",
        "    sheets_com_problemas = {}\n",
        "\n",
        "    for ano in anos_a_verificar:\n",
        "        caminho_arquivo = os.path.join(diretorio, f\"{ano}.xlsx\")\n",
        "        try:\n",
        "            xls = pd.ExcelFile(caminho_arquivo)\n",
        "            nomes_sheets = xls.sheet_names\n",
        "\n",
        "            sheets_faltantes = [f\"{mes}-{ano}\" for mes in meses_esperados if f\"{mes}-{ano}\" not in nomes_sheets]\n",
        "\n",
        "            if sheets_faltantes:\n",
        "                log_mensagem(f\"No arquivo '{ano}.xlsx', sheets faltantes: {sheets_faltantes}\", tipo='AVISO')\n",
        "                sheets_com_problemas[ano] = sheets_faltantes\n",
        "            else:\n",
        "                log_mensagem(f\"Arquivo '{ano}.xlsx' está completo com as 12 sheets mensais.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log_mensagem(f\"Não foi possível ler o arquivo '{ano}.xlsx': {e}\", tipo='ERRO')\n",
        "            sheets_com_problemas[ano] = ['ERRO DE LEITURA']\n",
        "\n",
        "    if not sheets_com_problemas:\n",
        "        log_mensagem(\"Todos os arquivos verificados possuem as sheets mensais corretas.\")\n",
        "\n",
        "    return sheets_com_problemas\n",
        "\n",
        "def validar_esquema_colunas(diretorio, anos_a_verificar):\n",
        "    \"\"\"\n",
        "    Verifica se todas as planilhas em todos os arquivos têm as mesmas colunas.\n",
        "\n",
        "    Args:\n",
        "        diretorio (str): Caminho para a pasta com os arquivos XLSX.\n",
        "        anos_a_verificar (list): Lista de anos cujos arquivos serão verificados.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dicionário com planilhas que apresentam colunas divergentes.\n",
        "    \"\"\"\n",
        "    log_mensagem(\"\\nIniciando validação de esquema de colunas...\")\n",
        "    if not anos_a_verificar:\n",
        "        log_mensagem(\"Nenhum ano para verificar. Pulando validação de colunas.\", tipo='AVISO')\n",
        "        return {}\n",
        "\n",
        "    try:\n",
        "        primeiro_ano = anos_a_verificar[0]\n",
        "        caminho_primeiro_arquivo = os.path.join(diretorio, f\"{primeiro_ano}.xlsx\")\n",
        "        xls_primeiro = pd.ExcelFile(caminho_primeiro_arquivo)\n",
        "        df_referencia = pd.read_excel(caminho_primeiro_arquivo, sheet_name=xls_primeiro.sheet_names[0])\n",
        "        colunas_padrao = set(df_referencia.columns)\n",
        "        log_mensagem(f\"Esquema de referência definido com {len(colunas_padrao)} colunas a partir de '{primeiro_ano}.xlsx'.\")\n",
        "    except Exception as e:\n",
        "        log_mensagem(f\"Não foi possível definir o esquema de referência: {e}\", tipo='ERRO')\n",
        "        return {'ERRO_REFERENCIA': str(e)}\n",
        "\n",
        "    colunas_com_problemas = {}\n",
        "    for ano in anos_a_verificar:\n",
        "        caminho_arquivo = os.path.join(diretorio, f\"{ano}.xlsx\")\n",
        "        try:\n",
        "            xls = pd.ExcelFile(caminho_arquivo)\n",
        "            for sheet in xls.sheet_names:\n",
        "                df_atual = pd.read_excel(caminho_arquivo, sheet_name=sheet)\n",
        "                colunas_atuais = set(df_atual.columns)\n",
        "\n",
        "                if colunas_atuais != colunas_padrao:\n",
        "                    diferenca = colunas_atuais.symmetric_difference(colunas_padrao)\n",
        "                    log_mensagem(f\"Arquivo '{ano}.xlsx', sheet '{sheet}' possui colunas divergentes: {diferenca}\", tipo='AVISO')\n",
        "                    colunas_com_problemas[f\"{ano}-{sheet}\"] = list(diferenca)\n",
        "        except Exception as e:\n",
        "            log_mensagem(f\"Erro ao processar a sheet '{sheet}' do arquivo '{ano}.xlsx': {e}\", tipo='ERRO')\n",
        "            colunas_com_problemas[f\"{ano}-{sheet}\"] = ['ERRO DE LEITURA']\n",
        "\n",
        "    if not colunas_com_problemas:\n",
        "        log_mensagem(\"Todas as planilhas verificadas possuem o mesmo esquema de colunas.\")\n",
        "\n",
        "    return colunas_com_problemas"
      ],
      "metadata": {
        "id": "TxOhW2uLyd3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5. Execução da Pipeline de Validação\n",
        "\n"
      ],
      "metadata": {
        "id": "MsDruEkoyfZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A célula a seguir orquestra a execução das funções de validação. Ela define os parâmetros necessários (como o intervalo de anos esperado) e chama cada função na sequência correta. Ao final, um resumo do processo de validação é gerado no arquivo de log, concluindo formalmente a Etapa 1."
      ],
      "metadata": {
        "id": "lmFEmY-qy-YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE VALIDAÇÃO DA ETAPA 1 ---\")\n",
        "\n",
        "ANOS_ESPERADOS = list(range(2006, 2024))\n",
        "log_mensagem(f\"Anos esperados para o projeto: de {ANOS_ESPERADOS[0]} a {ANOS_ESPERADOS[-1]}\")\n",
        "\n",
        "anos_encontrados, anos_faltantes = validar_anos(RAW_DATA_DIR, ANOS_ESPERADOS)\n",
        "\n",
        "if not anos_encontrados:\n",
        "    log_mensagem(\"Nenhum arquivo de dados encontrado. A validação não pode continuar.\", tipo='ERRO CRÍTICO')\n",
        "else:\n",
        "    sheets_faltantes_dict = validar_meses_em_sheets(RAW_DATA_DIR, anos_encontrados)\n",
        "\n",
        "    colunas_divergentes_dict = validar_esquema_colunas(RAW_DATA_DIR, anos_encontrados)\n",
        "\n",
        "    log_mensagem(\"\\n--- RESUMO DA VALIDAÇÃO DA ETAPA 1 ---\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de anos faltantes: {len(anos_faltantes)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de arquivos com sheets mensais faltantes: {len(sheets_faltantes_dict)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de planilhas com esquema de colunas divergente: {len(colunas_divergentes_dict)}\", tipo='SUMÁRIO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE VALIDAÇÃO DA ETAPA 1 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gryf_TZtygNi",
        "outputId": "bb528a0b-820c-4a50-d771-ff84f509c14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 20:44:19 - \n",
            "--- INICIANDO PIPELINE DE VALIDAÇÃO DA ETAPA 1 ---\n",
            "[INFO] 20:44:19 - Anos esperados para o projeto: de 2006 a 2023\n",
            "[INFO] 20:44:19 - Iniciando validação de anos existentes...\n",
            "[INFO] 20:44:20 - Todos os anos esperados foram encontrados.\n",
            "[INFO] 20:44:20 - Anos encontrados: [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
            "[INFO] 20:44:20 - \n",
            "Iniciando validação de sheets mensais...\n",
            "[INFO] 20:44:24 - Arquivo '2006.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:28 - Arquivo '2007.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:32 - Arquivo '2008.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:36 - Arquivo '2009.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:40 - Arquivo '2010.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:43 - Arquivo '2011.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:47 - Arquivo '2012.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:50 - Arquivo '2013.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:52 - Arquivo '2014.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:55 - Arquivo '2015.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:44:57 - Arquivo '2016.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:00 - Arquivo '2017.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:02 - Arquivo '2018.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:07 - Arquivo '2019.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:11 - Arquivo '2020.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:17 - Arquivo '2021.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:23 - Arquivo '2022.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:28 - Arquivo '2023.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:38 - Arquivo '2024.xlsx' está completo com as 12 sheets mensais.\n",
            "[INFO] 20:45:38 - Todos os arquivos verificados possuem as sheets mensais corretas.\n",
            "[INFO] 20:45:38 - \n",
            "Iniciando validação de esquema de colunas...\n",
            "[INFO] 20:45:41 - Esquema de referência definido com 28 colunas a partir de '2006.xlsx'.\n",
            "[INFO] 20:59:12 - Todas as planilhas verificadas possuem o mesmo esquema de colunas.\n",
            "[SUMÁRIO] 20:59:12 - \n",
            "--- RESUMO DA VALIDAÇÃO DA ETAPA 1 ---\n",
            "[SUMÁRIO] 20:59:12 - Total de anos faltantes: 0\n",
            "[SUMÁRIO] 20:59:12 - Total de arquivos com sheets mensais faltantes: 0\n",
            "[SUMÁRIO] 20:59:12 - Total de planilhas com esquema de colunas divergente: 0\n",
            "[INFO] 20:59:12 - \n",
            "--- FINALIZAÇÃO DA PIPELINE DE VALIDAÇÃO DA ETAPA 1 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 2: Preparação da Tabela de Consulta Geográfica"
      ],
      "metadata": {
        "id": "gWMw2yhE4d7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Introdução\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uFy52cEx4hQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após validar a integridade estrutural dos dados na Etapa 1, o próximo passo é enriquecê-los. A localização de um imóvel é um dos fatores mais preponderantes na determinação de seu valor. No entanto, os dados brutos contêm apenas o Código de Endereçamento Postal (CEP), uma informação que, por si só, possui baixo valor preditivo direto.\n",
        "\n",
        "Esta etapa foca na criação de um artefato de dados fundamental: uma tabela de consulta que traduz cada CEP único encontrado nos dados transacionais em informações geográficas de alto valor, como bairro e coordenadas (latitude e longitude).\n"
      ],
      "metadata": {
        "id": "0EPb2XlNxjap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Objetivos\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QFmDi1P_xKo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Extrair e Unificar CEPs:** Ler todos os arquivos de dados brutos para extrair e consolidar uma lista completa de todos os CEPs mencionados.\n",
        "2.  **Mapear com Dados Geográficos:** Cruzar a lista de CEPs únicos com um arquivo mestre de geolocalização para obter os atributos geográficos correspondentes.\n",
        "3.  **Criar a Tabela de Consulta:** Gerar um arquivo `.csv` limpo e otimizado que servirá como a principal fonte de dados geográficos para a Etapa 3. Este arquivo conterá a relação `CEP -> Bairro, Longitude, Latitude`.\n",
        "4.  **Identificar CEPs Faltantes:** Produzir um relatório de CEPs que existem nos dados transacionais mas não foram encontrados no arquivo mestre, possibilitando uma análise futura da qualidade dos dados.\n",
        "5.  **Manter Rastreabilidade:** Registrar todo o processo em um arquivo de log detalhado.\n"
      ],
      "metadata": {
        "id": "nCP9PIG8xlbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Metodologia\n"
      ],
      "metadata": {
        "id": "3VOQDrKQxL9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "O processo consiste em duas fases principais. Primeiro, uma varredura completa nos arquivos `.xlsx` é realizada para extrair todos os CEPs. Segundo, esses CEPs são comparados a uma base de dados geográfica externa (`sp_data.xlsx`). O resultado desta comparação é materializado em dois arquivos de saída: a tabela de consulta com os CEPs mapeados e uma lista com os CEPs que não puderam ser encontrados, garantindo que nenhuma informação seja perdida."
      ],
      "metadata": {
        "id": "kIJMiCuDxonJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "RAW_DATA_DIR = os.path.join(DRIVE_BASE_PATH, 'xlsx_tratados')\n",
        "MASTER_GEO_FILE_PATH = os.path.join(DRIVE_BASE_PATH, 'dados_cep', 'sp_data.xlsx')\n",
        "\n",
        "STAGE_02_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_02_geo_lookup')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_02_OUTPUT_DIR, 'log_etapa_2.txt')\n",
        "GEO_LOOKUP_TABLE_PATH = os.path.join(STAGE_02_OUTPUT_DIR, 'ceps_geolocalizados.csv')\n",
        "MISSING_CEPS_PATH = os.path.join(STAGE_02_OUTPUT_DIR, 'ceps_faltantes_no_mestre.csv')\n",
        "\n",
        "os.makedirs(STAGE_02_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 2 criado em: {STAGE_02_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpDN26TX4lsX",
        "outputId": "f3fdd03e-96b8-4c1d-b1f9-5908d5715dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório de saída da Etapa 2 criado em: /content/drive/MyDrive/dados_tcc/stage_02_geo_lookup\n",
            "Arquivo de log inicializado em: /content/drive/MyDrive/dados_tcc/stage_02_geo_lookup/log_etapa_2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Funções de Processamento\n",
        "\n"
      ],
      "metadata": {
        "id": "bPx2TKJ54muN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lógica desta etapa está contida nas seguintes funções, que garantem um fluxo de trabalho modular e claro.\n",
        "\n",
        "-   **`log_mensagem`**: A mesma função auxiliar da etapa anterior para registro de logs.\n",
        "-   **`extrair_ceps_de_todos_arquivos`**: Itera sobre todos os arquivos `.xlsx` e extrai a coluna de CEP de cada planilha.\n",
        "-   **`criar_tabela_consulta_geo`**: Pega a lista de CEPs extraídos, compara com o arquivo mestre e gera os DataFrames de saída."
      ],
      "metadata": {
        "id": "P2a_23ksxs1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"\n",
        "    Registra uma mensagem formatada no console e no arquivo de log.\n",
        "    \"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def extrair_ceps_de_todos_arquivos(diretorio):\n",
        "    \"\"\"\n",
        "    Extrai todos os CEPs não nulos de todos os arquivos XLSX no diretório.\n",
        "\n",
        "    Args:\n",
        "        diretorio (str): Caminho para a pasta com os arquivos XLSX.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Um DataFrame com uma única coluna 'CEP' contendo todos os CEPs extraídos.\n",
        "    \"\"\"\n",
        "    log_mensagem(\"Iniciando extração de CEPs de todos os arquivos brutos...\")\n",
        "    lista_de_ceps = []\n",
        "\n",
        "    arquivos_xlsx = [f for f in os.listdir(diretorio) if f.endswith('.xlsx')]\n",
        "    log_mensagem(f\"Encontrados {len(arquivos_xlsx)} arquivos .xlsx para processar.\")\n",
        "\n",
        "    for arquivo in arquivos_xlsx:\n",
        "        caminho_arquivo = os.path.join(diretorio, arquivo)\n",
        "        try:\n",
        "            xls = pd.ExcelFile(caminho_arquivo)\n",
        "            for sheet in xls.sheet_names:\n",
        "                df = pd.read_excel(caminho_arquivo, sheet_name=sheet)\n",
        "                if 'CEP' in df.columns:\n",
        "                    ceps_validos = pd.to_numeric(df['CEP'], errors='coerce').dropna().astype(int)\n",
        "                    lista_de_ceps.extend(ceps_validos.tolist())\n",
        "            log_mensagem(f\"Processado com sucesso o arquivo: {arquivo}\")\n",
        "        except Exception as e:\n",
        "            log_mensagem(f\"Erro ao processar o arquivo {arquivo}: {e}\", tipo='ERRO')\n",
        "\n",
        "    log_mensagem(f\"Extração finalizada. Total de {len(lista_de_ceps)} registros de CEP encontrados (com duplicatas).\")\n",
        "    return pd.DataFrame(lista_de_ceps, columns=['CEP'])\n",
        "\n",
        "def criar_tabela_consulta_geo(df_ceps_extraidos, caminho_geo_mestre):\n",
        "    \"\"\"\n",
        "    Compara os CEPs extraídos com o arquivo mestre e cria a tabela de consulta.\n",
        "\n",
        "    Args:\n",
        "        df_ceps_extraidos (pd.DataFrame): DataFrame com os CEPs dos arquivos brutos.\n",
        "        caminho_geo_mestre (str): Caminho para o arquivo sp_data.xlsx.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Uma tupla contendo (df_lookup_table, df_ceps_faltantes) ou (None, None) em caso de erro.\n",
        "    \"\"\"\n",
        "    log_mensagem(\"Iniciando criação da tabela de consulta geográfica...\")\n",
        "\n",
        "    try:\n",
        "        ceps_unicos_extraidos = set(df_ceps_extraidos['CEP'].unique())\n",
        "        log_mensagem(f\"Total de CEPs únicos a serem mapeados: {len(ceps_unicos_extraidos)}\")\n",
        "\n",
        "        df_geo_mestre = pd.read_excel(caminho_geo_mestre)\n",
        "        df_geo_mestre['CEP'] = pd.to_numeric(df_geo_mestre['CEP'], errors='coerce').dropna().astype(int)\n",
        "        ceps_unicos_mestre = set(df_geo_mestre['CEP'].unique())\n",
        "        log_mensagem(f\"Arquivo mestre de geolocalização carregado com {len(ceps_unicos_mestre)} CEPs únicos.\")\n",
        "\n",
        "        ceps_encontrados = ceps_unicos_extraidos.intersection(ceps_unicos_mestre)\n",
        "        ceps_faltantes = ceps_unicos_extraidos.difference(ceps_unicos_mestre)\n",
        "\n",
        "        log_mensagem(f\"CEPs encontrados no arquivo mestre: {len(ceps_encontrados)}\")\n",
        "        log_mensagem(f\"CEPs não encontrados no arquivo mestre: {len(ceps_faltantes)}\", tipo='AVISO' if ceps_faltantes else 'INFO')\n",
        "\n",
        "        df_lookup_table = df_geo_mestre[df_geo_mestre['CEP'].isin(ceps_encontrados)].drop_duplicates(subset=['CEP']).reset_index(drop=True)\n",
        "        log_mensagem(f\"Tabela de consulta criada com {len(df_lookup_table)} entradas únicas.\")\n",
        "\n",
        "        df_ceps_faltantes = pd.DataFrame(list(ceps_faltantes), columns=['CEP'])\n",
        "\n",
        "        return df_lookup_table, df_ceps_faltantes\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        log_mensagem(f\"Arquivo mestre de geolocalização não encontrado em: {caminho_geo_mestre}\", tipo='ERRO CRÍTICO')\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        log_mensagem(f\"Ocorreu um erro inesperado ao criar a tabela de consulta: {e}\", tipo='ERRO CRÍTICO')\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "fiDuzoh84ndu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. Execução da Pipeline de Preparação Geográfica\n",
        "\n"
      ],
      "metadata": {
        "id": "ZylhmOkX4onF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A célula final orquestra a execução das funções. Ela chama a função de extração, passa os resultados para a função de criação da tabela de consulta e, por fim, salva os artefatos gerados (`.csv`) no diretório de saída da etapa. O log é atualizado em cada passo, garantindo a rastreabilidade completa do processo."
      ],
      "metadata": {
        "id": "S5LNFeRfxxOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\")\n",
        "\n",
        "df_ceps_extraidos = extrair_ceps_de_todos_arquivos(RAW_DATA_DIR)\n",
        "\n",
        "if df_ceps_extraidos.empty:\n",
        "    log_mensagem(\"Nenhum CEP foi extraído. A etapa não pode continuar.\", tipo='ERRO CRÍTICO')\n",
        "else:\n",
        "    df_tabela_final, df_faltantes = criar_tabela_consulta_geo(df_ceps_extraidos, MASTER_GEO_FILE_PATH)\n",
        "\n",
        "    if df_tabela_final is not None:\n",
        "        try:\n",
        "            df_tabela_final.to_csv(GEO_LOOKUP_TABLE_PATH, index=False, encoding='utf-8')\n",
        "            log_mensagem(f\"Tabela de consulta geográfica salva em: {GEO_LOOKUP_TABLE_PATH}\")\n",
        "\n",
        "            if not df_faltantes.empty:\n",
        "                df_faltantes.to_csv(MISSING_CEPS_PATH, index=False, encoding='utf-8')\n",
        "                log_mensagem(f\"Arquivo com CEPs não encontrados salvo em: {MISSING_CEPS_PATH}\", tipo='AVISO')\n",
        "            else:\n",
        "                log_mensagem(\"Não houve CEPs faltantes para salvar.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log_mensagem(f\"Erro ao salvar os arquivos de saída: {e}\", tipo='ERRO')\n",
        "\n",
        "log_mensagem(\"\\n--- RESUMO DA PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\", tipo='SUMÁRIO')\n",
        "if 'df_tabela_final' in locals() and df_tabela_final is not None:\n",
        "    log_mensagem(f\"Total de CEPs únicos mapeados e salvos: {len(df_tabela_final)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de CEPs não encontrados no mestre: {len(df_faltantes)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Artefatos salvos em: {STAGE_02_OUTPUT_DIR}\", tipo='SUMÁRIO')\n",
        "else:\n",
        "    log_mensagem(\"A etapa foi concluída com erros e nenhum artefato foi gerado.\", tipo='SUMÁRIO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR2IETuL4pP3",
        "outputId": "8fb687d1-a4fa-4ab8-9a80-87a46d7be44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 21:02:07 - \n",
            "--- INICIANDO PIPELINE DE PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\n",
            "[INFO] 21:02:07 - Iniciando extração de CEPs de todos os arquivos brutos...\n",
            "[INFO] 21:02:07 - Encontrados 19 arquivos .xlsx para processar.\n",
            "[INFO] 21:02:46 - Processado com sucesso o arquivo: 2007.xlsx\n",
            "[INFO] 21:03:27 - Processado com sucesso o arquivo: 2010.xlsx\n",
            "[INFO] 21:04:06 - Processado com sucesso o arquivo: 2012.xlsx\n",
            "[INFO] 21:04:36 - Processado com sucesso o arquivo: 2015.xlsx\n",
            "[INFO] 21:05:03 - Processado com sucesso o arquivo: 2016.xlsx\n",
            "[INFO] 21:05:32 - Processado com sucesso o arquivo: 2017.xlsx\n",
            "[INFO] 21:06:04 - Processado com sucesso o arquivo: 2018.xlsx\n",
            "[INFO] 21:06:47 - Processado com sucesso o arquivo: 2008.xlsx\n",
            "[INFO] 21:07:27 - Processado com sucesso o arquivo: 2009.xlsx\n",
            "[INFO] 21:08:08 - Processado com sucesso o arquivo: 2013.xlsx\n",
            "[INFO] 21:08:44 - Processado com sucesso o arquivo: 2014.xlsx\n",
            "[INFO] 21:09:21 - Processado com sucesso o arquivo: 2006.xlsx\n",
            "[INFO] 21:10:02 - Processado com sucesso o arquivo: 2011.xlsx\n",
            "[INFO] 21:10:40 - Processado com sucesso o arquivo: 2019.xlsx\n",
            "[INFO] 21:11:20 - Processado com sucesso o arquivo: 2020.xlsx\n",
            "[INFO] 21:12:22 - Processado com sucesso o arquivo: 2021.xlsx\n",
            "[INFO] 21:13:21 - Processado com sucesso o arquivo: 2022.xlsx\n",
            "[INFO] 21:14:25 - Processado com sucesso o arquivo: 2023.xlsx\n",
            "[INFO] 21:15:43 - Processado com sucesso o arquivo: 2024.xlsx\n",
            "[INFO] 21:15:43 - Extração finalizada. Total de 2369346 registros de CEP encontrados (com duplicatas).\n",
            "[INFO] 21:15:43 - Iniciando criação da tabela de consulta geográfica...\n",
            "[INFO] 21:15:43 - Total de CEPs únicos a serem mapeados: 40815\n",
            "[INFO] 21:15:55 - Arquivo mestre de geolocalização carregado com 57849 CEPs únicos.\n",
            "[INFO] 21:15:55 - CEPs encontrados no arquivo mestre: 40287\n",
            "[AVISO] 21:15:55 - CEPs não encontrados no arquivo mestre: 528\n",
            "[INFO] 21:15:55 - Tabela de consulta criada com 40287 entradas únicas.\n",
            "[INFO] 21:15:55 - Tabela de consulta geográfica salva em: /content/drive/MyDrive/dados_tcc/stage_02_geo_lookup/ceps_geolocalizados.csv\n",
            "[AVISO] 21:15:55 - Arquivo com CEPs não encontrados salvo em: /content/drive/MyDrive/dados_tcc/stage_02_geo_lookup/ceps_faltantes_no_mestre.csv\n",
            "[SUMÁRIO] 21:15:55 - \n",
            "--- RESUMO DA PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\n",
            "[SUMÁRIO] 21:15:55 - Total de CEPs únicos mapeados e salvos: 40287\n",
            "[SUMÁRIO] 21:15:55 - Total de CEPs não encontrados no mestre: 528\n",
            "[SUMÁRIO] 21:15:55 - Artefatos salvos em: /content/drive/MyDrive/dados_tcc/stage_02_geo_lookup\n",
            "[INFO] 21:15:55 - \n",
            "--- FINALIZAÇÃO DA PIPELINE DE PREPARAÇÃO GEOGRÁFICA DA ETAPA 2 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 3: Construção e Limpeza do Dataset Principal"
      ],
      "metadata": {
        "id": "A3rOAsoL-tAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Introdução\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t7qQyoZk-uuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com os dados brutos validados (Etapa 1) e a tabela de consulta geográfica preparada (Etapa 2), esta etapa foca em construir o dataset principal do projeto. O objetivo é transformar a coleção de arquivos anuais `.xlsx` em um único DataFrame, estruturado, limpo e enriquecido.\n",
        "\n",
        "Este processo é o coração da preparação de dados, pois unifica informações de múltiplas fontes (arquivos anuais e a tabela geográfica) em uma única tabela coesa, que servirá como base para toda a engenharia de features e modelagem subsequentes."
      ],
      "metadata": {
        "id": "PQK4PPqux1PI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.2. Objetivos\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8OTv5IIJxU_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Consolidar Dados Transacionais:** Ler todos os arquivos e planilhas de dados brutos e uni-los em um único DataFrame.\n",
        "2.  **Padronizar o Esquema:** Aplicar um mapeamento de colunas consistente para padronizar os nomes e facilitar a manipulação dos dados.\n",
        "3.  **Realizar Limpeza Primária:** Converter os tipos de dados para formatos adequados (numérico, data), remover duplicatas e aplicar filtros de domínio para reter apenas os imóveis de interesse (residenciais).\n",
        "4.  **Enriquecer com Dados Geográficos:** Utilizar a tabela de consulta criada na Etapa 2 para adicionar informações de Bairro, Longitude e Latitude a cada transação, através de uma junção pela coluna `CEP`.\n",
        "5.  **Armazenar o Dataset Consolidado:** Salvar o DataFrame final em um formato eficiente (Parquet) para otimizar o armazenamento e a velocidade de leitura nas próximas etapas.\n"
      ],
      "metadata": {
        "id": "EsIyfx4Qx21_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Metodologia\n"
      ],
      "metadata": {
        "id": "ZZvWSdu1xWWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A metodologia desta etapa segue um fluxo de trabalho sequencial:\n",
        "1.  **Leitura e Concatenação:** Todos os dados são lidos em memória e empilhados.\n",
        "2.  **Limpeza e Filtragem:** São aplicadas transformações para garantir a qualidade e a relevância dos dados.\n",
        "3.  **Junção (Merge):** Os dados transacionais são cruzados com os dados geográficos.\n",
        "4.  **Salvamento:** O produto final é persistido em disco.\n",
        "\n",
        "Cada passo é meticulosamente registrado em log para garantir total transparência e reprodutibilidade."
      ],
      "metadata": {
        "id": "Jxveo3TOx46z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "RAW_DATA_DIR = os.path.join(DRIVE_BASE_PATH, 'xlsx_tratados')\n",
        "GEO_LOOKUP_TABLE_PATH = os.path.join(DRIVE_BASE_PATH, 'stage_02_geo_lookup', 'ceps_geolocalizados.csv')\n",
        "\n",
        "STAGE_03_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_03_consolidated_data')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_03_OUTPUT_DIR, 'log_etapa_3.txt')\n",
        "CONSOLIDATED_DATASET_PATH = os.path.join(STAGE_03_OUTPUT_DIR, 'dataset_consolidado.parquet')\n",
        "\n",
        "os.makedirs(STAGE_03_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE CONSTRUÇÃO DO DATASET PRINCIPAL DA ETAPA 3 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 3 criado em: {STAGE_03_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR-Ni3TT-xPd",
        "outputId": "8c439d86-d525-4709-f3a2-5d5b02f3790e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório de saída da Etapa 3 criado em: /content/drive/MyDrive/dados_tcc/stage_03_consolidated_data\n",
            "Arquivo de log inicializado em: /content/drive/MyDrive/dados_tcc/stage_03_consolidated_data/log_etapa_3.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Mapeamento de Colunas e Funções de Processamento\n"
      ],
      "metadata": {
        "id": "QqsxHt6u-yri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para garantir um esquema de dados padronizado e legível, definimos um dicionário de mapeamento que traduz os nomes originais das colunas para um formato `snake_case`. As funções abaixo encapsulam a lógica para consolidação, limpeza e enriquecimento do dataset."
      ],
      "metadata": {
        "id": "vCPSu50Vx8lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAPEAMENTO_COLUNAS = {\n",
        "    'N° do Cadastro (SQL)': 'n_cadastro',\n",
        "    'Nome do Logradouro': 'nome_logradouro',\n",
        "    'Número': 'numero',\n",
        "    'Complemento': 'complemento',\n",
        "    'Bairro': 'bairro_original',\n",
        "    'Referência': 'referencia',\n",
        "    'CEP': 'cep',\n",
        "    'Natureza de Transação': 'natureza_transacao',\n",
        "    'Valor de Transação (declarado pelo contribuinte)': 'valor_transacao',\n",
        "    'Data de Transação': 'data_transacao',\n",
        "    'Valor Venal de Referência': 'valor_venal_referencia',\n",
        "    'Proporção Transmitida (%)': 'proporcao_transmitida',\n",
        "    'Valor Venal de Referência (proporcional)': 'valor_venal_referencia_proporcional',\n",
        "    'Base de Cálculo adotada': 'base_calculo_original',\n",
        "    'Tipo de Financiamento': 'tipo_financiamento',\n",
        "    'Valor Financiado': 'valor_financiado',\n",
        "    'Cartório de Registro': 'cartorio_de_registro',\n",
        "    'Matrícula do Imóvel': 'matricula_imovel',\n",
        "    'Situação do SQL': 'situacao_no_sql',\n",
        "    'Área do Terreno (m2)': 'area_terreno',\n",
        "    'Testada (m)': 'testada',\n",
        "    'Fração Ideal': 'fracao_ideal',\n",
        "    'Área Construída (m2)': 'area_construida',\n",
        "    'Uso (IPTU)': 'uso_iptu',\n",
        "    'Descrição do uso (IPTU)': 'descricao_uso_iptu',\n",
        "    'Padrão (IPTU)': 'padrao_iptu',\n",
        "    'Descrição do padrão (IPTU)': 'descricao_padrao_iptu',\n",
        "    'ACC (IPTU)': 'acc_iptu'\n",
        "}\n",
        "\n",
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def consolidar_dados_brutos(diretorio, mapeamento):\n",
        "    \"\"\"Lê, renomeia e consolida todos os arquivos XLSX em um único DataFrame.\"\"\"\n",
        "    log_mensagem(\"Iniciando consolidação dos dados brutos...\")\n",
        "    lista_dfs = []\n",
        "    for arquivo in [f for f in os.listdir(diretorio) if f.endswith('.xlsx')]:\n",
        "        ano = int(arquivo.split('.')[0])\n",
        "        caminho_arquivo = os.path.join(diretorio, arquivo)\n",
        "        try:\n",
        "            xls = pd.ExcelFile(caminho_arquivo)\n",
        "            for sheet in xls.sheet_names:\n",
        "                df_sheet = pd.read_excel(caminho_arquivo, sheet_name=sheet)\n",
        "                df_sheet.columns = df_sheet.columns.str.strip()\n",
        "                df_sheet = df_sheet.rename(columns=mapeamento)\n",
        "                df_sheet['ANO'] = ano\n",
        "                df_sheet['MES_ANO'] = f\"{ano}-{sheet.split('-')[0]}\"\n",
        "                lista_dfs.append(df_sheet)\n",
        "            log_mensagem(f\"Arquivo '{arquivo}' consolidado com sucesso.\")\n",
        "        except Exception as e:\n",
        "            log_mensagem(f\"Erro ao processar o arquivo {arquivo}: {e}\", tipo='ERRO')\n",
        "\n",
        "    df_consolidado = pd.concat(lista_dfs, ignore_index=True)\n",
        "    log_mensagem(f\"Consolidação concluída. Total de {len(df_consolidado)} registros.\")\n",
        "    return df_consolidado\n",
        "\n",
        "def limpar_e_filtrar_dataset(df):\n",
        "    \"\"\"Aplica a limpeza primária e os filtros de domínio no dataset.\"\"\"\n",
        "    log_mensagem(\"Iniciando limpeza primária e filtragem do dataset...\")\n",
        "\n",
        "    registros_antes = len(df)\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    log_mensagem(f\"{registros_antes - len(df)} linhas duplicadas foram removidas.\")\n",
        "\n",
        "    registros_antes = len(df)\n",
        "    filtro_residencial = df['descricao_padrao_iptu'].str.contains(\n",
        "        'RESIDENCIAL VERTICAL|RESIDENCIAL HORIZONTAL',\n",
        "        case=False,\n",
        "        na=False\n",
        "    )\n",
        "    df = df[filtro_residencial].copy()\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    log_mensagem(f\"{registros_antes - len(df)} registros não residenciais foram removidos.\")\n",
        "\n",
        "    df['data_transacao'] = pd.to_datetime(df['data_transacao'], format='%d/%m/%Y', errors='coerce')\n",
        "    log_mensagem(\"Tipos de dados de colunas de data foram convertidos.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def enriquecer_com_geo(df_principal, caminho_lookup):\n",
        "    \"\"\"Enriquece o dataset principal com os dados geográficos e remove colunas desnecessárias.\"\"\"\n",
        "    log_mensagem(\"Iniciando enriquecimento com dados geográficos...\")\n",
        "\n",
        "    log_mensagem(f\"Colunas disponíveis para merge: {df_principal.columns.tolist()}\", tipo='DEBUG')\n",
        "\n",
        "    try:\n",
        "        df_lookup = pd.read_csv(caminho_lookup)\n",
        "        log_mensagem(f\"Tabela de consulta geográfica carregada com {len(df_lookup)} registros.\")\n",
        "    except FileNotFoundError:\n",
        "        log_mensagem(f\"Arquivo de lookup não encontrado em '{caminho_lookup}'. Processo interrompido.\", tipo='ERRO CRÍTICO')\n",
        "        return None\n",
        "\n",
        "    df_principal['cep'] = pd.to_numeric(df_principal['cep'], errors='coerce')\n",
        "    df_principal = df_principal.dropna(subset=['cep'])\n",
        "    df_principal['cep'] = df_principal['cep'].astype(int)\n",
        "\n",
        "    registros_antes = len(df_principal)\n",
        "    df_enriquecido = pd.merge(df_principal, df_lookup, on='cep', how='left')\n",
        "\n",
        "    registros_mapeados = df_enriquecido['Bairro'].notna().sum()\n",
        "    percentual_mapeado = (registros_mapeados / registros_antes) * 100 if registros_antes > 0 else 0\n",
        "    log_mensagem(f\"{registros_mapeados} de {registros_antes} registros ({percentual_mapeado:.2f}%) foram enriquecidos com dados geográficos.\")\n",
        "\n",
        "    registros_antes = len(df_enriquecido)\n",
        "    df_enriquecido = df_enriquecido.dropna(subset=['Bairro']).reset_index(drop=True)\n",
        "    log_mensagem(f\"{registros_antes - len(df_enriquecido)} registros sem correspondência geográfica foram removidos.\")\n",
        "\n",
        "    colunas_para_remover = [\n",
        "        'nome_logradouro',\n",
        "        'numero',\n",
        "        'complemento',\n",
        "        'bairro_original',\n",
        "        'referencia',\n",
        "        'cartorio_de_registro',\n",
        "        'situacao_no_sql'\n",
        "    ]\n",
        "    df_enriquecido = df_enriquecido.drop(columns=colunas_para_remover, errors='ignore')\n",
        "    log_mensagem(f\"Colunas desnecessárias removidas do dataset: {colunas_para_remover}\")\n",
        "\n",
        "    return df_enriquecido"
      ],
      "metadata": {
        "id": "oSW6bAsG-zaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. Execução da Pipeline de Construção do Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "DRXhLM5t-1JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A célula a seguir orquestra a execução de todas as funções definidas. O processo sequencial garante que os dados brutos sejam primeiramente consolidados, depois limpos e filtrados, e finalmente enriquecidos. O resultado é um único arquivo Parquet, que representa o estado mais atual e completo dos dados até o momento."
      ],
      "metadata": {
        "id": "1_32_Aw1x_wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE CONSTRUÇÃO DO DATASET DA ETAPA 3 ---\")\n",
        "\n",
        "df_consolidado = consolidar_dados_brutos(RAW_DATA_DIR, MAPEAMENTO_COLUNAS)\n",
        "\n",
        "if not df_consolidado.empty:\n",
        "    df_limpo = limpar_e_filtrar_dataset(df_consolidado)\n",
        "\n",
        "    df_final = enriquecer_com_geo(df_limpo, GEO_LOOKUP_TABLE_PATH)\n",
        "\n",
        "    if df_final is not None and not df_final.empty:\n",
        "        try:\n",
        "            df_final.to_parquet(CONSOLIDATED_DATASET_PATH)\n",
        "            log_mensagem(f\"Dataset consolidado e enriquecido salvo com sucesso em: {CONSOLIDATED_DATASET_PATH}\")\n",
        "        except Exception as e:\n",
        "            log_mensagem(f\"Falha ao salvar o arquivo Parquet: {e}\", tipo='ERRO')\n",
        "    else:\n",
        "        log_mensagem(\"O DataFrame final está vazio ou nulo. Nenhum arquivo foi salvo.\", tipo='AVISO')\n",
        "else:\n",
        "    log_mensagem(\"Nenhum dado foi consolidado. Processo interrompido.\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- RESUMO DA CONSTRUÇÃO DO DATASET DA ETAPA 3 ---\", tipo='SUMÁRIO')\n",
        "if 'df_final' in locals() and df_final is not None:\n",
        "    log_mensagem(f\"Total de registros no dataset final: {len(df_final)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de colunas no dataset final: {len(df_final.columns)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Intervalo de anos presente: de {df_final['ANO'].min()} a {df_final['ANO'].max()}\", tipo='SUMÁRIO')\n",
        "else:\n",
        "    log_mensagem(\"A etapa foi concluída com erros e o dataset consolidado não foi gerado.\", tipo='SUMÁRIO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE CONSTRUÇÃO DO DATASET DA ETAPA 3 ---\")"
      ],
      "metadata": {
        "id": "8gDDp9wK-1sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 4: Engenharia de Features"
      ],
      "metadata": {
        "id": "UYTXCqqYb7yO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Introdução\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWBPwsyEb_Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Engenharia de Features é o processo de utilizar o conhecimento de domínio para criar variáveis que auxiliam os algoritmos de machine learning a preverem melhor. O dataset consolidado na Etapa 3, embora limpo e estruturado, contém colunas que não representam diretamente o \"valor real\" de um imóvel em um determinado momento no tempo.\n",
        "\n",
        "Nesta etapa, vamos transformar esses dados brutos. As ações mais importantes serão recalcular a base de cálculo do imposto, que servirá como nosso proxy para o valor do imóvel, e ajustar este valor pela inflação acumulada ao longo dos anos. Este valor corrigido se tornará a nossa **variável-alvo** (o que queremos prever). Adicionalmente, criaremos outras features relevantes, como o preço por metro quadrado.\n"
      ],
      "metadata": {
        "id": "KVAf0yeHyHb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Objetivos\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WPrWXyjeyEPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Aplicar Tipos de Dados Corretos:** Garantir que todas as colunas numéricas utilizadas nos cálculos estejam no formato correto para evitar erros.\n",
        "2.  **Recalcular a Base de Cálculo:** Implementar a regra de negócio que define a `base_calculo` como o valor máximo entre o valor da transação e o valor venal de referência proporcional.\n",
        "3.  **Criar a Variável-Alvo:** Ajustar a `base_calculo` pela inflação, trazendo todos os valores para uma base monetária equivalente (2024), criando a coluna `base_calculo_corrigida_2024`.\n",
        "4.  **Tratar Outliers:** Remover valores extremos da variável-alvo para aumentar a estabilidade e a performance do modelo.\n",
        "5.  **Criar Features Derivadas:** Calcular novas variáveis, como o preço por metro quadrado construído e do terreno, com base no valor corrigido.\n",
        "6.  **Finalizar o Dataset:** Remover colunas intermediárias que foram usadas para os cálculos, resultando em um dataset enxuto e pronto para a próxima etapa.\n"
      ],
      "metadata": {
        "id": "vZRxsYUhyJF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Metodologia\n"
      ],
      "metadata": {
        "id": "PXcPL4FXyFyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "O processo será executado através de uma série de funções que transformam o DataFrame sequencialmente. Cada função encapsula uma lógica de negócio ou uma transformação estatística específica. O resultado é um novo arquivo Parquet, contendo o dataset com todas as features de engenharia, e um log detalhado de todas as transformações realizadas."
      ],
      "metadata": {
        "id": "7XUXU9q3yKg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_03_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_03_consolidated_data')\n",
        "CONSOLIDATED_DATASET_PATH = os.path.join(STAGE_03_OUTPUT_DIR, 'dataset_consolidado.parquet')\n",
        "\n",
        "STAGE_04_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_04_feature_engineered')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_04_OUTPUT_DIR, 'log_etapa_4.txt')\n",
        "FEATURE_ENGINEERED_DATASET_PATH = os.path.join(STAGE_04_OUTPUT_DIR, 'dataset_com_features.parquet')\n",
        "\n",
        "os.makedirs(STAGE_04_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE ENGENHARIA DE FEATURES DA ETAPA 4 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 4 criado em: {STAGE_04_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "q3nHpXYFcBnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Funções de Engenharia de Features\n",
        "\n"
      ],
      "metadata": {
        "id": "ehs1I14hcCyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As funções a seguir implementam o pipeline de transformações desta etapa. Elas são projetadas para serem aplicadas em sequência, cada uma adicionando ou modificando features no dataset."
      ],
      "metadata": {
        "id": "48w1auKkyNnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def aplicar_tipos_e_limpeza(df):\n",
        "    \"\"\"Garante que as colunas numéricas estejam no formato correto para cálculo.\"\"\"\n",
        "    log_mensagem(\"Iniciando conversão de tipos de dados numéricos...\")\n",
        "    colunas_numericas = [\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'valor_financiado',\n",
        "        'area_terreno', 'testada', 'fracao_ideal', 'area_construida'\n",
        "    ]\n",
        "    for col in colunas_numericas:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    log_mensagem(\"Conversão de tipos concluída.\")\n",
        "    return df\n",
        "\n",
        "def calcular_base_de_calculo(df):\n",
        "    \"\"\"Recalcula a base de cálculo com base nas regras de negócio.\"\"\"\n",
        "    log_mensagem(\"Iniciando recálculo da base de cálculo...\")\n",
        "\n",
        "    df['valor_venal_referencia_proporcional'] = np.where(\n",
        "        (df['proporcao_transmitida'] > 0) & (df['proporcao_transmitida'] <= 1),\n",
        "        df['valor_venal_referencia'],\n",
        "        df['valor_venal_referencia'] * (df['proporcao_transmitida'] / 100)\n",
        "    )\n",
        "\n",
        "    df['base_calculo'] = df[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "\n",
        "    log_mensagem(\"Base de cálculo recalculada com sucesso.\")\n",
        "    return df\n",
        "\n",
        "def corrigir_pela_inflacao(df):\n",
        "    \"\"\"Ajusta a base de cálculo pela inflação, criando a variável-alvo.\"\"\"\n",
        "    log_mensagem(\"Iniciando correção de valores pela inflação...\")\n",
        "    inflacao_anual = {\n",
        "        2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91,\n",
        "        2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06,\n",
        "        2022: 5.79, 2023: 4.62, 2024: 4.83,\n",
        "    }\n",
        "\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "\n",
        "    for ano in anos_ordenados:\n",
        "        fatores_correcao[ano] = fator_acumulado\n",
        "        fator_acumulado *= fatores_anual[ano]\n",
        "\n",
        "    df['fator_correcao'] = df['ANO'].map(fatores_correcao)\n",
        "    df['base_calculo_corrigida_2024'] = (df['base_calculo'] * df['fator_correcao']).round(2)\n",
        "\n",
        "    log_mensagem(\"Variável-alvo 'base_calculo_corrigida_2024' criada com sucesso.\")\n",
        "    log_mensagem(f\"Estatísticas da variável-alvo (antes de tratar outliers): \"\n",
        "                 f\"Média={df['base_calculo_corrigida_2024'].mean():.2f}, \"\n",
        "                 f\"Mediana={df['base_calculo_corrigida_2024'].median():.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def remover_outliers(df, coluna, quantil_inferior=0.05, quantil_superior=0.90):\n",
        "    \"\"\"Remove outliers de uma coluna com base em quantis.\"\"\"\n",
        "    log_mensagem(f\"Iniciando remoção de outliers da coluna '{coluna}'...\")\n",
        "    registros_antes = len(df)\n",
        "\n",
        "    limite_inferior = df[coluna].quantile(quantil_inferior)\n",
        "    limite_superior = df[coluna].quantile(quantil_superior)\n",
        "\n",
        "    df_filtrado = df[(df[coluna] >= limite_inferior) & (df[coluna] <= limite_superior)].copy()\n",
        "\n",
        "    registros_removidos = registros_antes - len(df_filtrado)\n",
        "    log_mensagem(f\"{registros_removidos} registros ({registros_removidos/registros_antes:.2%}) foram removidos como outliers.\")\n",
        "    log_mensagem(f\"Dados mantidos entre R$ {limite_inferior:,.2f} e R$ {limite_superior:,.2f}.\")\n",
        "\n",
        "    return df_filtrado\n",
        "\n",
        "def criar_features_de_preco(df):\n",
        "    \"\"\"Cria features de preço por metro quadrado.\"\"\"\n",
        "    log_mensagem(\"Criando features de preço por m²...\")\n",
        "    df['preco_m2_construido_corrigido_2024'] = (df['base_calculo_corrigida_2024'] / df['area_construida']).replace([np.inf, -np.inf], np.nan)\n",
        "    df['preco_m2_terreno_corrigido_2024'] = (df['base_calculo_corrigida_2024'] / df['area_terreno']).replace([np.inf, -np.inf], np.nan)\n",
        "    log_mensagem(\"Features de preço por m² criadas.\")\n",
        "    return df\n",
        "\n",
        "def finalizar_dataset(df):\n",
        "    \"\"\"Remove colunas intermediárias e desnecessárias.\"\"\"\n",
        "    log_mensagem(\"Iniciando limpeza final do dataset...\")\n",
        "    colunas_para_remover = [\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original',\n",
        "        'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    colunas_existentes_para_remover = [col for col in colunas_para_remover if col in df.columns]\n",
        "\n",
        "    df_final = df.drop(columns=colunas_existentes_para_remover)\n",
        "    log_mensagem(f\"Colunas removidas: {colunas_existentes_para_remover}\")\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "xgZqHEIbcDp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Execução da Pipeline de Engenharia de Features\n",
        "\n"
      ],
      "metadata": {
        "id": "2CQVOAaRcFCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A célula a seguir orquestra a aplicação de todas as funções de engenharia de features. O processo começa carregando o dataset consolidado da Etapa 3 e aplica cada transformação de forma sequencial, registrando o impacto de cada passo. O resultado é o dataset que será efetivamente usado para treinar e avaliar os modelos de machine learning."
      ],
      "metadata": {
        "id": "G2hxU5nfyQtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE ENGENHARIA DE FEATURES DA ETAPA 4 ---\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet(CONSOLIDATED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset consolidado carregado com {len(df)} registros e {len(df.columns)} colunas.\")\n",
        "\n",
        "    df = aplicar_tipos_e_limpeza(df)\n",
        "    df = calcular_base_de_calculo(df)\n",
        "    df = corrigir_pela_inflacao(df)\n",
        "    df = remover_outliers(df, coluna='base_calculo_corrigida_2024', quantil_inferior=0.05, quantil_superior=0.95)\n",
        "    df = criar_features_de_preco(df)\n",
        "    df_final = finalizar_dataset(df)\n",
        "\n",
        "    df_final.to_parquet(FEATURE_ENGINEERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset com features de engenharia salvo com sucesso em: {FEATURE_ENGINEERED_DATASET_PATH}\")\n",
        "\n",
        "    log_mensagem(\"\\n--- RESUMO DA ENGENHARIA DE FEATURES DA ETAPA 4 ---\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de registros no dataset final: {len(df_final)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Total de colunas no dataset final: {len(df_final.columns)}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Colunas do dataset final: {df_final.columns.tolist()}\", tipo='SUMÁRIO')\n",
        "\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo de entrada não encontrado: {CONSOLIDATED_DATASET_PATH}\", tipo='ERRO CRÍTICO')\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado durante a execução da Etapa 4: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE ENGENHARIA DE FEATURES DA ETAPA 4 ---\")"
      ],
      "metadata": {
        "id": "azXk4U0_cFxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 5: Análise Exploratória de Dados (EDA)"
      ],
      "metadata": {
        "id": "GRJLxfTrljoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Introdução\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VeJ5Hjr1lmce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com um dataset limpo, consolidado e enriquecido com features de engenharia, esta etapa é dedicada à Análise Exploratória de Dados (EDA). O objetivo da EDA é desenvolver uma compreensão profunda sobre o dataset, descobrindo padrões, identificando anomalias, testando hipóteses e verificando suposições através de estatísticas descritivas e visualizações gráficas.\n",
        "\n",
        "Esta fase é fundamental para guiar a etapa de modelagem. Os insights aqui gerados — como a distribuição da nossa variável-alvo, a relação entre as features e as tendências temporais — informarão a seleção de algoritmos, a necessidade de transformações adicionais e a interpretação dos resultados do modelo."
      ],
      "metadata": {
        "id": "MKQmz0n5ym_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. Objetivos\n",
        "\n"
      ],
      "metadata": {
        "id": "q8hwMwWGyjku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Analisar a Variável-Alvo:** Investigar a distribuição, a escala e a dispersão da nossa variável-alvo, `base_calculo_corrigida_2024`.\n",
        "2.  **Explorar Correlações:** Calcular e visualizar a correlação entre as principais variáveis numéricas para entender suas inter-relações.\n",
        "3.  **Investigar Tendências Temporais:** Analisar como os preços dos imóveis (e o preço por m²) evoluíram ao longo dos anos.\n",
        "4.  **Comparar Categorias:** Comparar o comportamento dos preços entre diferentes tipos de imóveis (Residencial Horizontal vs. Vertical).\n",
        "5.  **Gerar Artefatos Visuais:** Salvar todos os gráficos gerados em um diretório específico para documentação e referência futura.\n"
      ],
      "metadata": {
        "id": "sK_kIgCIypG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3. Metodologia"
      ],
      "metadata": {
        "id": "_QQBiz4LylY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A metodologia consistirá em carregar o dataset preparado na Etapa 4 e aplicar uma sequência de técnicas de análise. Para cada análise, geraremos uma visualização (histograma, boxplot, heatmap, gráfico de barras, etc.) e a salvaremos como um arquivo de imagem (`.png`). Todo o processo, incluindo estatísticas chave, será registrado em um arquivo de log."
      ],
      "metadata": {
        "id": "Y9ptncKoyq7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_04_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_04_feature_engineered')\n",
        "FEATURE_ENGINEERED_DATASET_PATH = os.path.join(STAGE_04_OUTPUT_DIR, 'dataset_com_features.parquet')\n",
        "\n",
        "STAGE_05_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_05_eda')\n",
        "PLOTS_DIR = os.path.join(STAGE_05_OUTPUT_DIR, 'plots')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_05_OUTPUT_DIR, 'log_etapa_5.txt')\n",
        "\n",
        "os.makedirs(STAGE_05_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE ANÁLISE EXPLORATÓRIA DE DADOS DA ETAPA 5 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 5 criado em: {STAGE_05_OUTPUT_DIR}\")\n",
        "print(f\"Diretório de plots criado em: {PLOTS_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")\n",
        "\n",
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")"
      ],
      "metadata": {
        "id": "tKGwN3dGlywc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4. Carregamento dos Dados e Análise Descritiva\n"
      ],
      "metadata": {
        "id": "q423m2QelzuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Iniciamos carregando o dataset e realizando uma análise descritiva básica para entender suas dimensões e a distribuição da nossa variável-alvo."
      ],
      "metadata": {
        "id": "jD_VMtoAytkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"Iniciando a Etapa 5: Análise Exploratória de Dados.\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet(FEATURE_ENGINEERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset carregado com sucesso. Shape: {df.shape}\")\n",
        "\n",
        "    log_mensagem(\"--- Informações Gerais do DataFrame (df.info) ---\")\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        df.info(buf=f)\n",
        "\n",
        "    log_mensagem(\"\\n--- Estatísticas Descritivas (df.describe) ---\")\n",
        "    descricao_estatistica = df.describe().to_string()\n",
        "    log_mensagem(descricao_estatistica)\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    log_mensagem(f\"\\nAnalisando a variável-alvo: '{TARGET}'\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.histplot(df[TARGET], kde=True, bins=50)\n",
        "    plt.title(f'Distribuição de {TARGET}', fontsize=16)\n",
        "    plt.xlabel('Valor (R$ corrigido)')\n",
        "    plt.ylabel('Frequência')\n",
        "    plt.tight_layout()\n",
        "    path_hist = os.path.join(PLOTS_DIR, '01_distribuicao_variavel_alvo.png')\n",
        "    plt.savefig(path_hist)\n",
        "    plt.show()\n",
        "    log_mensagem(f\"Gráfico de distribuição da variável-alvo salvo em: {path_hist}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    sns.boxplot(x=df[TARGET])\n",
        "    plt.title(f'Boxplot de {TARGET}', fontsize=16)\n",
        "    plt.xlabel('Valor (R$ corrigido)')\n",
        "    plt.tight_layout()\n",
        "    path_box = os.path.join(PLOTS_DIR, '02_boxplot_variavel_alvo.png')\n",
        "    plt.savefig(path_box)\n",
        "    plt.show()\n",
        "    log_mensagem(f\"Boxplot da variável-alvo salvo em: {path_box}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo de entrada não encontrado: {FEATURE_ENGINEERED_DATASET_PATH}\", tipo='ERRO CRÍTICO')\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado: {e}\", tipo='ERRO CRÍTICO')"
      ],
      "metadata": {
        "id": "KmRlqNuYl0PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5. Análise de Correlação\n",
        "\n"
      ],
      "metadata": {
        "id": "hZyASgTPl1ST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise de correlação nos ajuda a entender a força e a direção da relação linear (Pearson) e monotônica (Spearman) entre as variáveis numéricas. Um heatmap é uma excelente ferramenta para visualizar essas matrizes de correlação."
      ],
      "metadata": {
        "id": "4gh-xOIKyyQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\nIniciando análise de correlação...\")\n",
        "\n",
        "colunas_para_corr = [\n",
        "    'base_calculo_corrigida_2024',\n",
        "    'valor_financiado',\n",
        "    'area_terreno',\n",
        "    'testada',\n",
        "    'fracao_ideal',\n",
        "    'area_construida',\n",
        "    'acc_iptu',\n",
        "    'ANO',\n",
        "    'preco_m2_construido_corrigido_2024',\n",
        "    'preco_m2_terreno_corrigido_2024'\n",
        "]\n",
        "colunas_existentes = [col for col in colunas_para_corr if col in df.columns]\n",
        "\n",
        "df_corr = df[colunas_existentes].copy()\n",
        "df_corr['acc_iptu'] = pd.to_numeric(df_corr['acc_iptu'], errors='coerce')\n",
        "\n",
        "\n",
        "def plot_heatmap(corr_matrix, title, filename):\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    path_full = os.path.join(PLOTS_DIR, filename)\n",
        "    plt.savefig(path_full)\n",
        "    plt.show()\n",
        "    log_mensagem(f\"Heatmap '{title}' salvo em: {path_full}\")\n",
        "\n",
        "corr_pearson = df_corr.corr(method='pearson')\n",
        "log_mensagem(\"\\n--- Matriz de Correlação de Pearson ---\")\n",
        "log_mensagem(corr_pearson.to_string())\n",
        "plot_heatmap(corr_pearson, 'Matriz de Correlação de Pearson', '03_heatmap_pearson.png')\n",
        "\n",
        "corr_spearman = df_corr.corr(method='spearman')\n",
        "log_mensagem(\"\\n--- Matriz de Correlação de Spearman ---\")\n",
        "log_mensagem(corr_spearman.to_string())\n",
        "plot_heatmap(corr_spearman, 'Matriz de Correlação de Spearman', '04_heatmap_spearman.png')"
      ],
      "metadata": {
        "id": "L1S_Zm-7l133"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6. Análise Temporal e Comparativa por Tipo de Imóvel\n"
      ],
      "metadata": {
        "id": "QjcthtWDl2x2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Analisamos a seguir a evolução anual do preço médio dos imóveis, segmentando por tipo (Residencial Horizontal e Vertical). Isso pode revelar tendências de valorização distintas para casas e apartamentos."
      ],
      "metadata": {
        "id": "5m4og7Amy1hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\nIniciando análise temporal e comparativa por tipo de imóvel...\")\n",
        "\n",
        "df_anual = df.groupby(['ANO', 'descricao_padrao_iptu'])['base_calculo_corrigida_2024'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(data=df_anual, x='ANO', y='base_calculo_corrigida_2024', hue='descricao_padrao_iptu')\n",
        "plt.title('Média Anual da Base de Cálculo Corrigida por Tipo de Imóvel', fontsize=16)\n",
        "plt.ylabel('Média da Base de Cálculo (R$ corrigido)')\n",
        "plt.xlabel('Ano da Transação')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "path_bar = os.path.join(PLOTS_DIR, '05_evolucao_anual_preco_por_tipo.png')\n",
        "plt.savefig(path_bar)\n",
        "plt.show()\n",
        "log_mensagem(f\"Gráfico de evolução anual salvo em: {path_bar}\")\n",
        "\n",
        "\n",
        "df_anual_m2 = df.groupby(['ANO', 'descricao_padrao_iptu'])['preco_m2_construido_corrigido_2024'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(data=df_anual_m2, x='ANO', y='preco_m2_construido_corrigido_2024', hue='descricao_padrao_iptu')\n",
        "plt.title('Média Anual do Preço/m² Construído por Tipo de Imóvel', fontsize=16)\n",
        "plt.ylabel('Média do Preço/m² (R$ corrigido)')\n",
        "plt.xlabel('Ano da Transação')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "path_bar_m2 = os.path.join(PLOTS_DIR, '06_evolucao_anual_preco_m2_por_tipo.png')\n",
        "plt.savefig(path_bar_m2)\n",
        "plt.show()\n",
        "log_mensagem(f\"Gráfico de evolução anual do preço por m² salvo em: {path_bar_m2}\")"
      ],
      "metadata": {
        "id": "Zs04qgM3l3bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.7. Conclusão da Análise Exploratória\n"
      ],
      "metadata": {
        "id": "I1S7baV2l4Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise exploratória foi concluída. Os gráficos e estatísticas gerados foram salvos e fornecem uma base sólida de conhecimento para a próxima etapa de preparação final e modelagem."
      ],
      "metadata": {
        "id": "CFXQ315Ty4Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_plots = len([f for f in os.listdir(PLOTS_DIR) if f.endswith('.png')])\n",
        "\n",
        "log_mensagem(\"\\n--- RESUMO DA ANÁLISE EXPLORATÓRIA DA ETAPA 5 ---\", tipo='SUMÁRIO')\n",
        "log_mensagem(f\"Total de gráficos gerados e salvos: {num_plots}\", tipo='SUMÁRIO')\n",
        "log_mensagem(f\"Todos os artefatos visuais foram salvos em: {PLOTS_DIR}\", tipo='SUMÁRIO')\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE ANÁLISE EXPLORATÓRIA DA ETAPA 5 ---\")"
      ],
      "metadata": {
        "id": "Hp4uWokWl5MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 6: Preparação Final dos Datasets para Modelagem"
      ],
      "metadata": {
        "id": "LzwWMV0Dn_nV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1. Introdução\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j5lQC4gkoCIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta etapa representa a transição final entre a análise de dados e a construção do modelo. Com base nos valiosos insights gerados na Etapa 5 (Análise Exploratória), tomaremos as decisões finais de seleção de features e estruturaremos os dados na forma exata que os algoritmos de machine learning irão consumir.\n",
        "\n",
        "A principal decisão estratégica, fundamentada pela EDA, é a de **não utilizar um modelo único**. As características e a dinâmica de precificação de imóveis verticais (apartamentos) e horizontais (casas) são marcadamente distintas. Portanto, prepararemos dois datasets separados e otimizados, um para cada tipo de imóvel, permitindo a criação de modelos especialistas na Etapa 7."
      ],
      "metadata": {
        "id": "PFuyfNRRzCHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2. Objetivos\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w9JWdT9Dy-4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Aplicar a Seleção Final de Features:** Remover colunas que, após a análise, foram consideradas de baixo valor preditivo ou com problemas de qualidade, como `valor_financiado`.\n",
        "2.  **Segmentar o Dataset:** Dividir o dataset principal em dois subconjuntos: um contendo apenas registros de `RESIDENCIAL VERTICAL` e outro de `RESIDENCIAL HORIZONTAL`.\n",
        "3.  **Otimizar Features por Segmento:** Remover colunas que são irrelevantes para um segmento específico (ex: remover `area_terreno` do dataset de apartamentos).\n",
        "4.  **Gerar os Datasets de Modelagem:** Salvar os dois datasets finais em formato Parquet, prontos para a etapa de treinamento."
      ],
      "metadata": {
        "id": "37aOg4vwzENz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3. Metodologia\n"
      ],
      "metadata": {
        "id": "QJ1AVyrRzAUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "O processo consistirá em carregar o dataset da Etapa 4, aplicar as remoções de colunas globais, segmentar os dados por tipo de imóvel, aplicar as remoções de colunas específicas de cada segmento e, por fim, salvar os dois artefatos de dados resultantes."
      ],
      "metadata": {
        "id": "z6mQD29NzF0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_04_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_04_feature_engineered')\n",
        "FEATURE_ENGINEERED_DATASET_PATH = os.path.join(STAGE_04_OUTPUT_DIR, 'dataset_com_features.parquet')\n",
        "\n",
        "STAGE_06_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_06_modeling_datasets')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'log_etapa_6.txt')\n",
        "MODELING_DATASET_VERTICAL_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'dataset_modelagem_vertical.parquet')\n",
        "MODELING_DATASET_HORIZONTAL_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'dataset_modelagem_horizontal.parquet')\n",
        "\n",
        "os.makedirs(STAGE_06_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE PREPARAÇÃO FINAL DOS DATASETS DA ETAPA 6 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 6 criado em: {STAGE_06_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")\n",
        "\n",
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")"
      ],
      "metadata": {
        "id": "71UJureZoE_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4. Carregamento e Preparação Global\n",
        "\n"
      ],
      "metadata": {
        "id": "og4_3ERkoF-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro, carregamos o dataset da Etapa 4 e aplicamos as decisões de limpeza que valem para ambos os tipos de imóvel, como a remoção da coluna `valor_financiado` e outras colunas que não serão utilizadas como features no modelo."
      ],
      "metadata": {
        "id": "3SJaXhZ4zJte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"Iniciando a Etapa 6: Preparação Final dos Datasets para Modelagem.\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet(FEATURE_ENGINEERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset da Etapa 4 carregado com sucesso. Shape inicial: {df.shape}\")\n",
        "\n",
        "    log_mensagem(\"Extraindo e convertendo as colunas Latitude e Longitude para tipo numérico...\")\n",
        "\n",
        "    df['Latitude'] = df['Latitude'].astype(str).str.extract(r'(-?\\d+\\.\\d+)', expand=False)\n",
        "    df['Longitude'] = df['Longitude'].astype(str).str.extract(r'(-?\\d+\\.\\d+)', expand=False)\n",
        "\n",
        "    df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
        "    df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
        "\n",
        "    linhas_antes = len(df)\n",
        "    df.dropna(subset=['Latitude', 'Longitude'], inplace=True)\n",
        "    log_mensagem(f\"{linhas_antes - len(df)} linhas removidas por coordenadas inválidas.\")\n",
        "\n",
        "    colunas_para_remover_global = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep',\n",
        "        'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento',\n",
        "        'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu',\n",
        "        'descricao_uso_iptu',\n",
        "        'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo'\n",
        "    ]\n",
        "\n",
        "    colunas_existentes_para_remover = [col for col in colunas_para_remover_global if col in df.columns]\n",
        "    df_limpo = df.drop(columns=colunas_existentes_para_remover)\n",
        "\n",
        "    log_mensagem(f\"Remoção de colunas globais concluída. Colunas removidas: {colunas_existentes_para_remover}\")\n",
        "    log_mensagem(f\"Shape do dataset após limpeza global: {df_limpo.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    df_limpo = None\n",
        "    log_mensagem(f\"Arquivo de entrada não encontrado: {FEATURE_ENGINEERED_DATASET_PATH}\", tipo='ERRO CRÍTICO')\n",
        "except Exception as e:\n",
        "    df_limpo = None\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado: {e}\", tipo='ERRO CRÍTICO')"
      ],
      "metadata": {
        "id": "vo0NKPHwoGwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5. Segmentação e Otimização por Tipo de Imóvel\n"
      ],
      "metadata": {
        "id": "e4avmbbCoH0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Este é o passo central desta etapa. Filtramos o dataset limpo para criar dois subconjuntos e, em seguida, otimizamos as features de cada um antes de salvá-los."
      ],
      "metadata": {
        "id": "meqmY7YOzL85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if df_limpo is not None:\n",
        "    log_mensagem(\"Limpando a coluna 'descricao_padrao_iptu' para garantir a correspondência exata.\")\n",
        "    df_limpo['descricao_padrao_iptu'] = df_limpo['descricao_padrao_iptu'].str.strip()\n",
        "    log_mensagem(\"\\n--- Segmentando e otimizando o dataset para Imóveis Verticais ---\")\n",
        "    df_vertical = df_limpo[df_limpo['descricao_padrao_iptu'] == 'RESIDENCIAL VERTICAL'].copy()\n",
        "    log_mensagem(f\"Encontrados {len(df_vertical)} registros de imóveis verticais.\")\n",
        "\n",
        "    colunas_para_remover_vertical = ['area_terreno', 'testada', 'preco_m2_terreno_corrigido_2024']\n",
        "    df_vertical = df_vertical.drop(columns=colunas_para_remover_vertical, errors='ignore')\n",
        "    log_mensagem(f\"Colunas específicas removidas do dataset vertical: {colunas_para_remover_vertical}\")\n",
        "    log_mensagem(f\"Shape final do dataset vertical: {df_vertical.shape}\")\n",
        "\n",
        "    df_vertical.to_parquet(MODELING_DATASET_VERTICAL_PATH)\n",
        "    log_mensagem(f\"Dataset para modelagem de imóveis verticais salvo em: {MODELING_DATASET_VERTICAL_PATH}\")\n",
        "\n",
        "    log_mensagem(\"\\n--- Segmentando e otimizando o dataset para Imóveis Horizontais ---\")\n",
        "    df_horizontal = df_limpo[df_limpo['descricao_padrao_iptu'] == 'RESIDENCIAL HORIZONTAL'].copy()\n",
        "    log_mensagem(f\"Encontrados {len(df_horizontal)} registros de imóveis horizontais.\")\n",
        "\n",
        "    colunas_para_remover_horizontal = ['fracao_ideal']\n",
        "    df_horizontal = df_horizontal.drop(columns=colunas_para_remover_horizontal, errors='ignore')\n",
        "    log_mensagem(f\"Colunas específicas removidas do dataset horizontal: {colunas_para_remover_horizontal}\")\n",
        "    log_mensagem(f\"Shape final do dataset horizontal: {df_horizontal.shape}\")\n",
        "\n",
        "    df_horizontal.to_parquet(MODELING_DATASET_HORIZONTAL_PATH)\n",
        "    log_mensagem(f\"Dataset para modelagem de imóveis horizontais salvo em: {MODELING_DATASET_HORIZONTAL_PATH}\")\n",
        "\n",
        "    log_mensagem(\"\\n--- RESUMO DA PREPARAÇÃO FINAL DA ETAPA 6 ---\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Dataset VERTICAL: {len(df_vertical)} linhas, {len(df_vertical.columns)} colunas.\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Colunas Finais (Vertical): {df_vertical.columns.tolist()}\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Dataset HORIZONTAL: {len(df_horizontal)} linhas, {len(df_horizontal.columns)} colunas.\", tipo='SUMÁRIO')\n",
        "    log_mensagem(f\"Colunas Finais (Horizontal): {df_horizontal.columns.tolist()}\", tipo='SUMÁRIO')\n",
        "\n",
        "else:\n",
        "    log_mensagem(\"Processo da Etapa 6 interrompido devido a erros no carregamento dos dados.\", tipo='AVISO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE PREPARAÇÃO FINAL DA ETAPA 6 ---\")"
      ],
      "metadata": {
        "id": "NG0VNDa7oIeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_horizontal.info()"
      ],
      "metadata": {
        "id": "XiBl6lwbuqCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 7a: Modelagem com Scikit-learn"
      ],
      "metadata": {
        "id": "5Gu3tpqPplxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1. Introdução\n",
        "\n"
      ],
      "metadata": {
        "id": "EJ98zQODqPCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bem-vindo à etapa de modelagem preditiva. Com os datasets de imóveis verticais e horizontais devidamente preparados e otimizados, o próximo passo é construir modelos de machine learning para prever a variável-alvo (`base_calculo_corrigida_2024`).\n",
        "\n",
        "Nesta primeira abordagem (7a), utilizaremos a biblioteca `scikit-learn`, um padrão da indústria para modelagem em Python. Construiremos uma pipeline de modelagem robusta que encapsula o pré-processamento de features e o treinamento de um modelo de regressão. O `RandomForestRegressor` foi escolhido como nosso algoritmo inicial por sua alta performance, robustez a outliers e capacidade de fornecer a importância das features.\n",
        "\n"
      ],
      "metadata": {
        "id": "kbnV6Wa8zPZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2. Objetivos\n",
        "\n"
      ],
      "metadata": {
        "id": "UlgTXuZszRFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Construir uma Pipeline Reutilizável:** Criar uma função que automatize o processo de treinamento e avaliação, incluindo a divisão dos dados, pré-processamento e treinamento do modelo.\n",
        "2.  **Implementar Divisão Temporal:** Separar os dados em conjuntos de treino, validação e teste com base no ano da transação, uma prática crucial para evitar vazamento de dados (*data leakage*) em problemas com componente temporal.\n",
        "3.  **Treinar Modelos Especialistas:** Aplicar a pipeline de modelagem separadamente para os datasets de imóveis verticais e horizontais.\n",
        "4.  **Avaliar a Performance:** Medir a acurácia dos modelos utilizando as métricas R² (Coeficiente de Determinação) e RMSE (Raiz do Erro Quadrático Médio).\n",
        "5.  **Analisar a Importância das Features:** Extrair e visualizar quais features mais contribuem para as previsões de cada modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ioz2pXnPzSZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3. Metodologia\n",
        "\n"
      ],
      "metadata": {
        "id": "li1PZMwdzTuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada um dos dois datasets, aplicaremos a mesma metodologia: os dados serão divididos por ano (Treino: 2006-2015, Validação: 2016-2020, Teste: 2021-2024). Uma pipeline do Scikit-learn cuidará do pré-processamento (como a codificação de variáveis categóricas com `OneHotEncoder`) e do treinamento do regressor. Os resultados de performance em cada conjunto de dados serão registrados para análise comparativa."
      ],
      "metadata": {
        "id": "f-BmKyTfzU0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 8)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_06_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_06_modeling_datasets')\n",
        "MODELING_DATASET_VERTICAL_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'dataset_modelagem_vertical.parquet')\n",
        "MODELING_DATASET_HORIZONTAL_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'dataset_modelagem_horizontal.parquet')\n",
        "\n",
        "STAGE_07a_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_07a_sklearn_modeling')\n",
        "PLOTS_DIR_7a = os.path.join(STAGE_07a_OUTPUT_DIR, 'plots')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_07a_OUTPUT_DIR, 'log_etapa_7a.txt')\n",
        "\n",
        "os.makedirs(STAGE_07a_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR_7a, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM (SKLEARN) DA ETAPA 7a ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 7a criado em: {STAGE_07a_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")\n",
        "\n",
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")"
      ],
      "metadata": {
        "id": "ibVbqbYWqR6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4. Definição da Pipeline de Modelagem\n"
      ],
      "metadata": {
        "id": "PciPZvw1qTF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para garantir a reprodutibilidade e evitar a duplicação de código, criamos uma função chamada `treinar_e_avaliar_modelo`. Esta função recebe um DataFrame e um nome para o modelo, e executa todo o fluxo de trabalho: divisão temporal, definição do pré-processamento, criação da pipeline, treinamento, avaliação e visualização da importância das features."
      ],
      "metadata": {
        "id": "lRnqNNRKzYei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treinar_e_avaliar_modelo(df, nome_modelo):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo de treinamento e avaliação para um dado DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): O dataset de modelagem.\n",
        "        nome_modelo (str): Um nome descritivo para o modelo (ex: 'Vertical', 'Horizontal').\n",
        "    \"\"\"\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO PARA O MODELO: {nome_modelo} (Versão Corrigida) ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "\n",
        "    leaky_features = ['preco_m2_construido_corrigido_2024', 'preco_m2_terreno_corrigido_2024']\n",
        "    features = [col for col in df.columns if col != TARGET and col not in leaky_features]\n",
        "    log_mensagem(f\"Features vazadas removidas do treinamento: {leaky_features}\")\n",
        "\n",
        "    X = df[features]\n",
        "    y = df[TARGET]\n",
        "\n",
        "    log_mensagem(\"Realizando divisão temporal: Treino (2006-2015), Validação (2016-2020), Teste (2021-2024)\")\n",
        "    X_train = X[X['ANO'].between(2006, 2015)]\n",
        "    y_train = y[X_train.index]\n",
        "    X_val = X[X['ANO'].between(2016, 2020)]\n",
        "    y_val = y[X_val.index]\n",
        "    X_test = X[X['ANO'].between(2021, 2024)]\n",
        "    y_test = y[X_test.index]\n",
        "    log_mensagem(f\"Tamanhos dos conjuntos: Treino={len(X_train)}, Validação={len(X_val)}, Teste={len(X_test)}\")\n",
        "\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numeric_features = X.select_dtypes(include=np.number).columns.drop('ANO').tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', 'passthrough', numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15)\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', model)\n",
        "    ])\n",
        "\n",
        "    log_mensagem(\"Iniciando o treinamento do RandomForestRegressor...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    log_mensagem(\"Treinamento concluído.\")\n",
        "\n",
        "    def avaliar(X_data, y_data, nome_conjunto):\n",
        "        preds = pipeline.predict(X_data)\n",
        "        r2 = r2_score(y_data, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_data, preds))\n",
        "        log_mensagem(f\"Performance no conjunto de {nome_conjunto}: R² = {r2:.4f} | RMSE = {rmse:,.2f}\", tipo='AVALIAÇÃO')\n",
        "\n",
        "    avaliar(X_train, y_train, 'Treino')\n",
        "    avaliar(X_val, y_val, 'Validação')\n",
        "    avaliar(X_test, y_test, 'Teste')\n",
        "\n",
        "    log_mensagem(\"Gerando gráfico de importância das features...\")\n",
        "    try:\n",
        "        feature_names = numeric_features + pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
        "        importances = pipeline.named_steps['regressor'].feature_importances_\n",
        "\n",
        "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
        "        top_features = feature_importance_df.head(20)\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.barplot(x='importance', y='feature', data=top_features, palette='viridis')\n",
        "        plt.title(f'Top 20 Features Mais Importantes - Modelo {nome_modelo}', fontsize=16)\n",
        "        plt.xlabel('Importância')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.tight_layout()\n",
        "        plot_path = os.path.join(PLOTS_DIR_7a, f'feature_importance_{nome_modelo.lower()}.png')\n",
        "        plt.savefig(plot_path)\n",
        "        plt.show()\n",
        "        log_mensagem(f\"Gráfico de importância das features salvo em: {plot_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_mensagem(f\"Não foi possível gerar o gráfico de importância de features: {e}\", tipo=\"ERRO\")\n",
        "\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "oFU-TDeSqT-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5. Execução para Imóveis Verticais\n",
        "\n"
      ],
      "metadata": {
        "id": "WqtMt2ISqV2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, aplicamos nossa pipeline de modelagem ao dataset de imóveis verticais (apartamentos)."
      ],
      "metadata": {
        "id": "c7k7typJza7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df_vertical = pd.read_parquet(MODELING_DATASET_VERTICAL_PATH)\n",
        "    treinar_e_avaliar_modelo(df_vertical, \"Vertical\")\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo não encontrado: {MODELING_DATASET_VERTICAL_PATH}\", tipo=\"ERRO CRÍTICO\")\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro ao processar o dataset vertical: {e}\", tipo=\"ERRO CRÍTICO\")"
      ],
      "metadata": {
        "id": "DEgKjIlgqXAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6. Execução para Imóveis Horizontais\n",
        "\n"
      ],
      "metadata": {
        "id": "fP3SSBYzqYyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repetimos o mesmo processo para o dataset de imóveis horizontais (casas), permitindo uma comparação direta da performance e das features mais importantes entre os dois modelos."
      ],
      "metadata": {
        "id": "O0ZXuZxhzeEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df_horizontal = pd.read_parquet(MODELING_DATASET_HORIZONTAL_PATH)\n",
        "    treinar_e_avaliar_modelo(df_horizontal, \"Horizontal\")\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo não encontrado: {MODELING_DATASET_HORIZONTAL_PATH}\", tipo=\"ERRO CRÍTICO\")\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro ao processar o dataset horizontal: {e}\", tipo=\"ERRO CRÍTICO\")"
      ],
      "metadata": {
        "id": "7AHqbSkCqZ_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.7. Conclusão da Etapa 7a\n",
        "\n"
      ],
      "metadata": {
        "id": "ZDAINsoAqcEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A modelagem com Scikit-learn foi concluída. Os resultados de performance e a importância de features para cada modelo foram devidamente registrados no log. Esta análise nos dá uma linha de base sólida para comparar com outras abordagens, como a que será explorada na Etapa 7b com H2O."
      ],
      "metadata": {
        "id": "0SVAbhDOzfxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM (SKLEARN) DA ETAPA 7a ---\", tipo=\"SUMÁRIO\")"
      ],
      "metadata": {
        "id": "JJe25j7UqeHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 7b: Modelagem com H2O AutoML"
      ],
      "metadata": {
        "id": "jiAJa-KzyFQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.8. Introdução\n",
        "\n"
      ],
      "metadata": {
        "id": "EhEC7NwFyHLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após estabelecermos um baseline de performance realista com o `RandomForestRegressor` do Scikit-learn, vamos agora explorar uma abordagem de modelagem automatizada. Nesta etapa, utilizaremos a plataforma **H2O.ai**, especificamente sua ferramenta de Machine Learning Automatizado (AutoML).\n",
        "\n",
        "O H2O AutoML treina automaticamente uma vasta gama de modelos (Gradient Boosting, Redes Neurais, Modelos Lineares, etc.) e realiza a otimização de hiperparâmetros, apresentando uma tabela de classificação (leaderboard) dos melhores modelos encontrados. O objetivo é verificar se conseguimos superar o nosso baseline de forma rápida e eficiente.\n",
        "\n"
      ],
      "metadata": {
        "id": "CLEnMAa1zlV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.9. Objetivos\n",
        "\n"
      ],
      "metadata": {
        "id": "oY-xz5rFzmdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Utilizar H2O AutoML:** Empregar a ferramenta para treinar múltiplos modelos nos nossos datasets.\n",
        "2.  **Manter a Divisão Temporal:** Garantir que o H2O utilize os mesmos conjuntos de treino e validação para uma comparação justa.\n",
        "3.  **Analisar o Leaderboard:** Avaliar a performance dos diferentes modelos treinados pelo H2O.\n",
        "4.  **Comparar com o Baseline:** Comparar o melhor modelo do H2O com o resultado do RandomForest da Etapa 7a."
      ],
      "metadata": {
        "id": "eDxtDWnaznq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o -q\n",
        "\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "STAGE_07b_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_07b_h2o_modeling')\n",
        "LOG_FILE_PATH_7b = os.path.join(STAGE_07b_OUTPUT_DIR, 'log_etapa_7b.txt')\n",
        "os.makedirs(STAGE_07b_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH_7b, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM (H2O) DA ETAPA 7b ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*50 + \"\\n\\n\")\n",
        "\n",
        "def log_mensagem_7b(mensagem, tipo='INFO'):\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH_7b, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "h2o.init()"
      ],
      "metadata": {
        "id": "8Z0-1oiWyI-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treinar_com_automl(df, nome_modelo):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo de treinamento com H2O AutoML.\n",
        "    \"\"\"\n",
        "    log_mensagem_7b(f\"--- INICIANDO TREINAMENTO H2O AutoML PARA O MODELO: {nome_modelo} ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    leaky_features = ['preco_m2_construido_corrigido_2024', 'preco_m2_terreno_corrigido_2024']\n",
        "    features_cols = [col for col in df.columns if col != TARGET and col not in leaky_features]\n",
        "\n",
        "    hf = h2o.H2OFrame(df[features_cols + [TARGET]])\n",
        "    log_mensagem_7b(\"DataFrame convertido para H2OFrame.\")\n",
        "\n",
        "    log_mensagem_7b(\"Realizando divisão temporal com a sintaxe correta do H2O...\")\n",
        "    train = hf[(hf['ANO'] >= 2006) & (hf['ANO'] <= 2015), :]\n",
        "    valid = hf[(hf['ANO'] >= 2016) & (hf['ANO'] <= 2020), :]\n",
        "    test = hf[(hf['ANO'] >= 2021) & (hf['ANO'] <= 2024), :]\n",
        "\n",
        "    log_mensagem_7b(f\"Divisão temporal realizada. Tamanhos: Treino={train.shape[0]}, Validação={valid.shape[0]}, Teste={test.shape[0]}\")\n",
        "\n",
        "    x = [col for col in features_cols if col != 'ANO']\n",
        "    y = TARGET\n",
        "\n",
        "    log_mensagem_7b(\"Iniciando H2O AutoML... (max_runtime_secs=600)\")\n",
        "    aml = H2OAutoML(\n",
        "        max_runtime_secs=600,\n",
        "        seed=42,\n",
        "        sort_metric=\"RMSE\"\n",
        "    )\n",
        "    aml.train(x=x, y=y, training_frame=train, validation_frame=valid, leaderboard_frame=test)\n",
        "\n",
        "    lb = aml.leaderboard\n",
        "    log_mensagem_7b(f\"\\n--- Leaderboard para o Modelo {nome_modelo} ---\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        log_mensagem_7b(lb.as_data_frame().to_string())\n",
        "\n",
        "    leader_model = aml.leader\n",
        "    log_mensagem_7b(f\"\\nMelhor modelo encontrado: {leader_model.model_id}\")\n",
        "\n",
        "    perf_test = leader_model.model_performance(test_data=test)\n",
        "    log_mensagem_7b(f\"\\n--- Performance do Melhor Modelo no Conjunto de Teste ({nome_modelo}) ---\")\n",
        "    log_mensagem_7b(str(perf_test))\n",
        "\n",
        "    log_mensagem_7b(f\"--- FINALIZADO TREINAMENTO H2O PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "8AtjAu0PyKmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem_7b(\"--- INICIANDO PROCESSAMENTO H2O ---\")\n",
        "try:\n",
        "    df_vertical = pd.read_parquet(MODELING_DATASET_VERTICAL_PATH)\n",
        "    treinar_com_automl(df_vertical, \"Vertical\")\n",
        "except Exception as e:\n",
        "    log_mensagem_7b(f\"Erro ao processar dataset vertical com H2O: {e}\", tipo=\"ERRO CRÍTICO\")\n",
        "\n",
        "try:\n",
        "    df_horizontal = pd.read_parquet(MODELING_DATASET_HORIZONTAL_PATH)\n",
        "    treinar_com_automl(df_horizontal, \"Horizontal\")\n",
        "except Exception as e:\n",
        "    log_mensagem_7b(f\"Erro ao processar dataset horizontal com H2O: {e}\", tipo=\"ERRO CRÍTICO\")\n",
        "\n",
        "log_mensagem_7b(\"--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM (H2O) DA ETAPA 7b ---\", tipo=\"SUMÁRIO\")\n",
        "h2o.cluster().shutdown()\n",
        "log_mensagem_7b(\"Cluster H2O desligado.\")"
      ],
      "metadata": {
        "id": "ljRyPD_8yL3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 8: Análise de Resultados, Conclusões e Próximos Passos"
      ],
      "metadata": {
        "id": "sQbI5FH67HZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1. Resumo do Projeto\n",
        "\n"
      ],
      "metadata": {
        "id": "T5xvGh4-7Jm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este projeto executou uma pipeline de ciência de dados de ponta a ponta com o objetivo de prever os valores de imóveis na cidade de São Paulo. O processo iniciou-se com a rigorosa validação e limpeza de dados transacionais de 2006 a 2024, seguida pela criação de uma variável-alvo robusta, `base_calculo_corrigida_2024`, que ajusta os valores dos imóveis pela inflação.\n",
        "\n",
        "A análise exploratória revelou dinâmicas de precificação distintas entre imóveis verticais (apartamentos) e horizontais (casas), fundamentando a decisão estratégica de desenvolver modelos especialistas para cada segmento. Finalmente, dois conjuntos de modelos foram treinados e avaliados: uma abordagem com `RandomForest` do Scikit-learn para estabelecer um baseline, e uma abordagem com `AutoML` da plataforma H2O para explorar o potencial de otimização automatizada.\n",
        "\n",
        "## 8.2. Análise Comparativa dos Modelos Finais\n",
        "\n",
        "A comparação de performance no conjunto de teste, que representa dados não vistos, é a principal medida do sucesso dos nossos modelos. Os resultados consolidados das Etapas 7a e 7b são apresentados abaixo:\n",
        "\n",
        "| Modelo             | Métrica | Scikit-learn (RandomForest) | H2O AutoML (Melhor Modelo) | Variação (H2O vs RF) |\n",
        "| ------------------ | :-----: | :-------------------------: | :------------------------: | :------------------: |\n",
        "| **Vertical** | **R²** |           0.7087            |       Aprox. 0.71       |      Equivalente       |\n",
        "| (Apartamentos)     | **RMSE**|        R$ 221.730,03        |       **R$ 219.427,32** |   **-1.04%** |\n",
        "|                    |           |                             |                            |                      |\n",
        "| **Horizontal** | **R²** |           0.3440            |         **0.3620** |      **+5.23%** |\n",
        "| (Casas)            | **RMSE**|        R$ 356.722,44        |       **R$ 351.790,77** |   **-1.38%** |\n",
        "\n",
        "#### Interpretação dos Resultados:\n",
        "\n",
        "* **Modelo Vertical (Apartamentos):** Ambos os modelos apresentaram um desempenho sólido e muito semelhante, com um **R² próximo a 0.71**. O melhor modelo do H2O (`GBM`) conseguiu uma redução marginal no erro (RMSE) de apenas 1.04%. Isso indica que o `RandomForest` já estava muito próximo do desempenho ótimo para as features disponíveis, demonstrando a robustez e a viabilidade da previsão para este segmento.\n",
        "\n",
        "* **Modelo Horizontal (Casas):** Para este segmento mais desafiador, o H2O (`StackedEnsemble`) também superou o `RandomForest`, com uma melhoria mais notável no R² (de 0.34 para 0.36). Contudo, a melhoria no RMSE ainda foi modesta (1.38%). Este resultado reforça a conclusão da análise exploratória: a dificuldade em prever o preço de casas está mais relacionada à ausência de features descritivas do que à limitação do algoritmo em si.\n",
        "\n",
        "## 8.3. Análise das Features Mais Importantes\n",
        "\n",
        "A análise dos modelos (após a remoção do *target leakage*) revelou um conjunto lógico de preditores de valor:\n",
        "\n",
        "* **Para ambos os modelos:** `area_construida`, `Latitude`, `Longitude`, `acc_iptu` (idade de construção) e `ANO` (ano da transação) figuraram entre as features mais importantes. Isso confirma que o **tamanho, a localização e a idade** são os pilares da precificação de imóveis.\n",
        "* **Para o modelo Horizontal:** A feature `area_terreno` demonstrou alta importância, validando a decisão de tratar as casas separadamente, pois a área do terreno é um fator crucial para este tipo de imóvel e irrelevante para apartamentos.\n",
        "\n",
        "## 8.4. Conclusões Principais do Estudo\n",
        "\n",
        "Ao final deste trabalho, podemos extrair quatro conclusões principais:\n",
        "\n",
        "1.  **Viabilidade da Previsão:** É comprovadamente viável criar um modelo de machine learning para estimar o valor de imóveis em São Paulo a partir de dados públicos, com um desempenho especialmente promissor para o segmento de apartamentos (R² ≈ 0.71).\n",
        "\n",
        "2.  **Importância Crítica da Segmentação:** A estratégia de segmentar o problema em dois modelos especialistas (casas e apartamentos) foi fundamental para a obtenção de resultados coerentes, dadas as diferentes dinâmicas de mercado e a relevância distinta das features para cada segmento.\n",
        "\n",
        "3.  **Desempenho Limitado pelas Features:** O estudo demonstrou que, para o segmento de casas, o ganho obtido ao se utilizar técnicas avançadas de AutoML foi marginal. Isso sugere fortemente que o \"teto\" de performance com os dados atuais foi alcançado, e que saltos significativos de precisão dependeriam da inclusão de novas variáveis mais descritivas.\n",
        "\n",
        "4.  **Baseline Sólido Estabelecido:** O projeto estabeleceu um baseline de performance robusto e reprodutível, tanto para modelos tradicionais (`RandomForest`) quanto para soluções de AutoML (`H2O`), que pode servir como ponto de partida para futuras pesquisas e refinamentos.\n",
        "\n",
        "## 8.5. Propostas para Trabalhos Futuros\n",
        "\n",
        "A natureza iterativa da ciência de dados permite que este trabalho seja expandido de várias maneiras promissoras:\n",
        "\n",
        "1.  **Engenharia de Features Geoespaciais (Clusterização):** Conforme discutido, a criação de uma feature categórica de \"zona\" ou \"região\" através de algoritmos de cluster como o K-Means é o próximo passo mais lógico. Combinar as coordenadas (`Latitude`, `Longitude`) com uma feature de `cluster_id` pode fornecer ao modelo tanto o contexto macro-regional quanto a precisão micro-local, com grande potencial para aumentar a acurácia de ambos os modelos.\n",
        "\n",
        "2.  **Enriquecimento com Dados Externos:** Para melhorar o modelo de casas, seria de grande valia buscar fontes de dados externas que possam conter informações não presentes no dataset atual, como:\n",
        "    * Dados demográficos por bairro (renda per capita, escolaridade).\n",
        "    * Índices de segurança pública.\n",
        "    * Proximidade a serviços (metrô, hospitais, parques).\n",
        "\n",
        "3.  **Otimização de Hiperparâmetros:** Realizar uma busca de hiperparâmetros mais exaustiva (ex: `GridSearchCV`) no modelo `RandomForest` para verificar se é possível igualar a performance do AutoML do H2O com um ajuste fino.\n",
        "\n",
        "4.  **Modelagem de Séries Temporais:** Explorar abordagens que tratem o problema mais explicitamente como uma série temporal, utilizando, por exemplo, a defasagem de preços de meses ou trimestres anteriores como features preditivas.\n",
        "\n",
        "**Parabéns! Você concluiu uma pipeline completa e complexa. O resultado é um trabalho de alta qualidade, com uma metodologia sólida e conclusões bem fundamentadas, pronto para ser apresentado.**"
      ],
      "metadata": {
        "id": "h1yjBHKbz0vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 9.1: Engenharia de Features - Clusterização Geográfica com K-Means"
      ],
      "metadata": {
        "id": "U2FQkeo17Ohh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1.1. Introdução\n",
        "\n",
        "Com base na discussão da Etapa 8, onde hipotetizamos que adicionar uma feature de \"zona\" ou \"região\" poderia melhorar a performance dos modelos, esta etapa é dedicada a criar essa nova feature. Utilizaremos o algoritmo de clusterização não supervisionado **K-Means** para agrupar todos os imóveis do nosso dataset em `k` clusters geograficamente distintos.\n",
        "\n",
        "O objetivo é transformar o par de coordenadas contínuas (`Latitude`, `Longitude`) em uma única e poderosa feature categórica (`localizacao_cluster`), que representará a macrorregião onde o imóvel está situado. Esta nova feature será testada em conjunto com as coordenadas e de forma isolada nas etapas de modelagem subsequentes (9.2 a 9.5).\n",
        "\n",
        "## 9.1.2. Objetivos\n",
        "\n",
        "1.  **Carregar o Dataset Consolidado:** Utilizar o dataset limpo da Etapa 3 como base, pois ele contém todos os imóveis antes de qualquer outra engenharia de features.\n",
        "2.  **Preparar os Dados Geoespaciais:** Filtrar o dataset para remover outliers geográficos (pontos fora da área de interesse de São Paulo) e padronizar a escala das coordenadas.\n",
        "3.  **Aplicar o K-Means:** Executar o algoritmo K-Means para agrupar os imóveis em 8 clusters distintos.\n",
        "4.  **Criar a Feature `localizacao_cluster`:** Adicionar uma nova coluna ao dataset com o rótulo do cluster atribuído a cada imóvel.\n",
        "5.  **Visualizar os Clusters:** Gerar um gráfico de dispersão para validar visualmente a coerência dos clusters formados.\n",
        "6.  **Salvar o Novo Dataset:** Armazenar o dataset, agora enriquecido com a feature de cluster, para uso nas próximas etapas.\n",
        "\n",
        "## 9.1.3. Metodologia\n",
        "\n",
        "A metodologia envolve o pré-processamento das coordenadas, incluindo a aplicação do `StandardScaler` para garantir que ambas as dimensões (latitude e longitude) tenham o mesmo peso no cálculo de distância. Em seguida, o K-Means é treinado para encontrar os centroides dos `k` clusters. Por fim, cada ponto de dado é rotulado com o cluster mais próximo, e o resultado é visualizado e salvo."
      ],
      "metadata": {
        "id": "nUWw7dz37ZYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 8)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_03_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_03_consolidated_data')\n",
        "CONSOLIDATED_DATASET_PATH = os.path.join(STAGE_03_OUTPUT_DIR, 'dataset_consolidado.parquet')\n",
        "\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "PLOTS_DIR_9_1 = os.path.join(STAGE_09_1_OUTPUT_DIR, 'plots')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'log_etapa_9_1.txt')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "os.makedirs(STAGE_09_1_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR_9_1, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE CLUSTERIZAÇÃO GEOGRÁFICA DA ETAPA 9.1 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 9.1 criado em: {STAGE_09_1_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")\n",
        "\n",
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")"
      ],
      "metadata": {
        "id": "En8ItETf7b3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1.4. Funções para Clusterização\n",
        "\n",
        "Para organizar o processo, a lógica foi dividida em funções para preparação dos dados e para a execução e visualização do K-Means."
      ],
      "metadata": {
        "id": "Z0DmIEPA7eZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def preparar_dados_geo(df):\n",
        "    \"\"\"\n",
        "    Filtra o DataFrame para conter apenas coordenadas válidas e dentro de uma caixa de limites.\n",
        "    \"\"\"\n",
        "    log_mensagem(\"Iniciando preparação dos dados geoespaciais.\")\n",
        "\n",
        "    log_mensagem(\"Aplicando extração de coordenadas a partir de strings com prefixo...\")\n",
        "\n",
        "    df['Latitude'] = df['Latitude'].astype(str)\n",
        "    df['Longitude'] = df['Longitude'].astype(str)\n",
        "\n",
        "    df['Latitude'] = df['Latitude'].str.extract(r'(-?\\d+\\.\\d+)', expand=False)\n",
        "    df['Longitude'] = df['Longitude'].str.extract(r'(-?\\d+\\.\\d+)', expand=False)\n",
        "\n",
        "    df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
        "    df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
        "\n",
        "    df_geo = df.dropna(subset=['Latitude', 'Longitude']).copy()\n",
        "    registros_antes = len(df)\n",
        "    log_mensagem(f\"{registros_antes - len(df_geo)} registros removidos por coordenadas nulas ou inválidas.\")\n",
        "\n",
        "    lat_bounds = (-24.1, -23.3)\n",
        "    lon_bounds = (-47.2, -46.2)\n",
        "\n",
        "    registros_antes = len(df_geo)\n",
        "    df_geo = df_geo[\n",
        "        (df_geo['Latitude'].between(lat_bounds[0], lat_bounds[1])) &\n",
        "        (df_geo['Longitude'].between(lon_bounds[0], lon_bounds[1]))\n",
        "    ].copy()\n",
        "    log_mensagem(f\"{registros_antes - len(df_geo)} registros removidos por estarem fora dos limites geográficos.\")\n",
        "    log_mensagem(f\"Total de registros válidos para clusterização: {len(df_geo)}\")\n",
        "\n",
        "    coordenadas = df_geo[['Latitude', 'Longitude']]\n",
        "    return df_geo, coordenadas\n",
        "\n",
        "def executar_kmeans_e_visualizar(df_geo, coordenadas, k):\n",
        "    \"\"\"\n",
        "    Executa o K-Means, adiciona os rótulos ao DataFrame e visualiza os clusters.\n",
        "    \"\"\"\n",
        "    log_mensagem(f\"Iniciando K-Means com k={k}...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    coordenadas_scaled = scaler.fit_transform(coordenadas)\n",
        "    log_mensagem(\"Coordenadas padronizadas com StandardScaler.\")\n",
        "\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "    df_geo['localizacao_cluster'] = kmeans.fit_predict(coordenadas_scaled)\n",
        "    log_mensagem(\"K-Means executado e rótulos de cluster adicionados ao dataset.\")\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.scatterplot(\n",
        "        data=df_geo.sample(n=min(50000, len(df_geo))),\n",
        "        x='Longitude',\n",
        "        y='Latitude',\n",
        "        hue='localizacao_cluster',\n",
        "        palette='viridis',\n",
        "        s=5,\n",
        "        legend='full'\n",
        "    )\n",
        "    plt.title(f'Clusters Geográficos de Imóveis (k={k})', fontsize=16)\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.legend(title='Cluster ID')\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(PLOTS_DIR_9_1, '01_mapa_clusters_kmeans.png')\n",
        "    plt.savefig(plot_path)\n",
        "    plt.show()\n",
        "    log_mensagem(f\"Gráfico de visualização dos clusters salvo em: {plot_path}\")\n",
        "\n",
        "    return df_geo"
      ],
      "metadata": {
        "id": "5f9RUU_g7fBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1.5. Execução da Pipeline de Clusterização\n",
        "\n",
        "A célula final orquestra todo o processo. Carregamos o dataset da Etapa 3, o preparamos para a análise geoespacial, executamos o K-Means e salvamos o resultado: um novo dataset enriquecido com a feature `localizacao_cluster`, que será a base para as próximas etapas de modelagem."
      ],
      "metadata": {
        "id": "CwAxB8Oz7gRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE CLUSTERIZAÇÃO GEOGRÁFICA (ETAPA 9.1) ---\")\n",
        "\n",
        "try:\n",
        "    df_consolidado = pd.read_parquet(CONSOLIDATED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset consolidado da Etapa 3 carregado com {len(df_consolidado)} registros.\")\n",
        "\n",
        "    K_CLUSTERS = 8\n",
        "\n",
        "    df_geo, coordenadas = preparar_dados_geo(df_consolidado)\n",
        "\n",
        "    if not df_geo.empty:\n",
        "        df_final_cluster = executar_kmeans_e_visualizar(df_geo, coordenadas, k=K_CLUSTERS)\n",
        "\n",
        "        df_final_cluster.to_parquet(CLUSTERED_DATASET_PATH)\n",
        "        log_mensagem(f\"Dataset com clusters salvo com sucesso em: {CLUSTERED_DATASET_PATH}\")\n",
        "\n",
        "        log_mensagem(\"\\n--- RESUMO DA CLUSTERIZAÇÃO (ETAPA 9.1) ---\", tipo='SUMÁRIO')\n",
        "        log_mensagem(f\"Total de registros no dataset final clusterizado: {len(df_final_cluster)}\", tipo='SUMÁRIO')\n",
        "        log_mensagem(f\"Número de clusters criados: {K_CLUSTERS}\", tipo='SUMÁRIO')\n",
        "        log_mensagem(f\"Contagem de imóveis por cluster:\\n{df_final_cluster['localizacao_cluster'].value_counts().to_string()}\", tipo='SUMÁRIO')\n",
        "    else:\n",
        "        log_mensagem(\"O DataFrame ficou vazio após a preparação dos dados geográficos. Nenhum cluster foi criado.\", tipo='AVISO')\n",
        "\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo de entrada não encontrado: {CONSOLIDATED_DATASET_PATH}\", tipo='ERRO CRÍTICO')\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 9.1: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE CLUSTERIZAÇÃO (ETAPA 9.1) ---\")"
      ],
      "metadata": {
        "id": "VQLDgKo-7hIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 9.2: Modelagem (Scikit-learn) com Features Híbridas (Lat/Lon + Cluster)"
      ],
      "metadata": {
        "id": "QNLgAML--XLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2.1. Introdução\n",
        "\n",
        "Esta etapa é o primeiro experimento para validar a eficácia da nossa nova feature de clusterização. O objetivo é treinar e avaliar novamente os modelos de `RandomForestRegressor` (um para imóveis verticais e outro para horizontais), mas desta vez fornecendo ao modelo um conjunto de features geoespaciais mais rico.\n",
        "\n",
        "A hipótese é que, ao fornecer tanto as coordenadas exatas (`Latitude`, `Longitude`) quanto a zona geográfica (`localizacao_cluster`), o modelo terá o \"melhor dos dois mundos\": o contexto da macrorregião e a especificidade da microlocalização. Esperamos que essa informação adicional se traduza em uma melhor performance preditiva, especialmente para o desafiador modelo de casas.\n",
        "\n",
        "## 9.2.2. Objetivos\n",
        "\n",
        "1.  **Preparar o Dataset Clusterizado:** Carregar o dataset da Etapa 9.1 e aplicar todo o pipeline de engenharia de features das Etapas 4 e 6 para torná-lo pronto para a modelagem.\n",
        "2.  **Treinar Novos Modelos:** Executar a pipeline de modelagem do Scikit-learn nos novos datasets (vertical e horizontal), agora contendo a feature `localizacao_cluster`.\n",
        "3.  **Avaliar e Comparar:** Medir a performance (R² e RMSE) dos novos modelos e comparar diretamente com os resultados do baseline da Etapa 7a.\n",
        "4.  **Analisar a Importância das Features:** Verificar a importância da nova feature `localizacao_cluster` em relação às outras.\n",
        "\n",
        "## 9.2.3. Metodologia\n",
        "\n",
        "A metodologia consistirá em um fluxo de trabalho autocontido. Primeiro, carregamos os dados clusterizados e aplicamos todas as transformações necessárias (cálculo da base de cálculo, correção de inflação, etc.). Em seguida, uma função de modelagem — uma versão atualizada daquela da Etapa 7a para tratar `localizacao_cluster` como uma categoria — será aplicada aos dados segmentados para treinar e avaliar os modelos."
      ],
      "metadata": {
        "id": "nMYV967AITJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "STAGE_09_2_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_2_sklearn_hybrid_geo')\n",
        "PLOTS_DIR_9_2 = os.path.join(STAGE_09_2_OUTPUT_DIR, 'plots')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_09_2_OUTPUT_DIR, 'log_etapa_9_2.txt')\n",
        "\n",
        "os.makedirs(STAGE_09_2_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR_9_2, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM HÍBRIDA (SKLEARN) DA ETAPA 9.2 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 9.2 criado em: {STAGE_09_2_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "juG_rjueIVa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2.4. Função de Modelagem\n",
        "\n",
        "A célula abaixo contém a função reutilizável para treinamento e avaliação do modelo. Ela é uma versão atualizada da função da Etapa 7a, projetada para identificar e tratar corretamente a nova feature `localizacao_cluster` como uma variável categórica."
      ],
      "metadata": {
        "id": "Q1PbEcNtIWcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def treinar_e_avaliar_modelo_com_cluster(df, nome_modelo):\n",
        "    \"\"\"Executa a pipeline de modelagem, tratando o cluster como categórico.\"\"\"\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO PARA O MODELO: {nome_modelo} (Lat/Lon + Cluster) ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    features = [col for col in df.columns if col != TARGET]\n",
        "    X = df[features]\n",
        "    y = df[TARGET]\n",
        "\n",
        "    X_train = X[X['ANO'].between(2006, 2015)]\n",
        "    y_train = y[X_train.index]\n",
        "    X_val = X[X['ANO'].between(2016, 2020)]\n",
        "    y_val = y[X_val.index]\n",
        "    X_test = X[X['ANO'].between(2021, 2024)]\n",
        "    y_test = y[X_test.index]\n",
        "\n",
        "    if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
        "        log_mensagem(\"Um ou mais conjuntos de dados (treino, validação, teste) está vazio. Abortando treinamento.\", tipo=\"ERRO\")\n",
        "        return\n",
        "\n",
        "    log_mensagem(f\"Tamanhos dos conjuntos: Treino={len(X_train)}, Validação={len(X_val)}, Teste={len(X_test)}\")\n",
        "\n",
        "    X['localizacao_cluster'] = X['localizacao_cluster'].astype('category')\n",
        "\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numeric_features = X.select_dtypes(include=np.number).columns.drop('ANO').tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', 'passthrough', numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15))\n",
        "    ])\n",
        "\n",
        "    log_mensagem(\"Iniciando o treinamento do RandomForestRegressor...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    log_mensagem(\"Treinamento concluído.\")\n",
        "\n",
        "    def avaliar(X_data, y_data, nome_conjunto):\n",
        "        preds = pipeline.predict(X_data)\n",
        "        r2 = r2_score(y_data, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_data, preds))\n",
        "        log_mensagem(f\"Performance no conjunto de {nome_conjunto}: R² = {r2:.4f} | RMSE = {rmse:,.2f}\", tipo='AVALIAÇÃO')\n",
        "\n",
        "    avaliar(X_train, y_train, 'Treino')\n",
        "    avaliar(X_val, y_val, 'Validação')\n",
        "    avaliar(X_test, y_test, 'Teste')\n",
        "\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "wdZntFmEIXEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2.5. Execução da Pipeline de Modelagem Híbrida\n",
        "\n",
        "A célula a seguir é o coração desta etapa. Ela orquestra todo o processo de forma linear e explícita:\n",
        "1.  Carrega o dataset enriquecido com os clusters.\n",
        "2.  Aplica todo o pipeline de engenharia de features (lógica da Etapa 4) e limpeza de colunas (lógica da Etapa 6) para deixar os dados prontos para a modelagem.\n",
        "3.  Verifica o estado do DataFrame limpo antes de prosseguir.\n",
        "4.  Segmenta os dados em \"Vertical\" e \"Horizontal\".\n",
        "5.  Chama a função `treinar_e_avaliar_modelo_com_cluster` para cada segmento."
      ],
      "metadata": {
        "id": "YSblF5pGIYRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE MODELAGEM HÍBRIDA (ETAPA 9.2) ---\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet(CLUSTERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset clusterizado da Etapa 9.1 carregado com {len(df)} registros.\")\n",
        "\n",
        "    log_mensagem(\"Iniciando engenharia de features...\")\n",
        "    colunas_numericas = ['valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida','area_terreno', 'testada', 'fracao_ideal', 'area_construida']\n",
        "    for col in colunas_numericas:\n",
        "      if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df['valor_venal_referencia_proporcional'] = df['valor_venal_referencia'] * (df['proporcao_transmitida'] / 100)\n",
        "    df['base_calculo'] = df[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "    inflacao_anual = {2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91, 2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06, 2022: 5.79, 2023: 4.62, 2024: 4.83}\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "    for ano in anos_ordenados:\n",
        "      fatores_correcao[ano] = fator_acumulado\n",
        "      fator_acumulado *= fatores_anual[ano]\n",
        "    df['fator_correcao'] = df['ANO'].map(fatores_correcao)\n",
        "    df['base_calculo_corrigida_2024'] = (df['base_calculo'] * df['fator_correcao']).round(2)\n",
        "    limite_inferior = df['base_calculo_corrigida_2024'].quantile(0.05)\n",
        "    limite_superior = df['base_calculo_corrigida_2024'].quantile(0.95)\n",
        "    df = df[(df['base_calculo_corrigida_2024'] >= limite_inferior) & (df['base_calculo_corrigida_2024'] <= limite_superior)].copy()\n",
        "    log_mensagem(\"Engenharia de features concluída.\")\n",
        "\n",
        "    log_mensagem(\"Iniciando limpeza final de colunas...\")\n",
        "    colunas_para_remover = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep', 'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento', 'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu', 'descricao_uso_iptu', 'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo',\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    colunas_existentes_para_remover = [col for col in colunas_para_remover if col in df.columns]\n",
        "    df = df.drop(columns=colunas_existentes_para_remover)\n",
        "    log_mensagem(f\"Limpeza final de colunas concluída. Shape atual: {df.shape}\")\n",
        "\n",
        "    log_mensagem(\"--- Informações do DataFrame Final Pronto para Modelagem ---\")\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        df.info(buf=f)\n",
        "\n",
        "    df['descricao_padrao_iptu'] = df['descricao_padrao_iptu'].str.strip()\n",
        "    df_vertical = df[df['descricao_padrao_iptu'] == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "    df_horizontal = df[df['descricao_padrao_iptu'] == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "\n",
        "    if not df_vertical.empty:\n",
        "        treinar_e_avaliar_modelo_com_cluster(df_vertical, \"Vertical\")\n",
        "    else:\n",
        "        log_mensagem(\"Dataset vertical vazio. Pulando treinamento.\", tipo=\"AVISO\")\n",
        "\n",
        "    if not df_horizontal.empty:\n",
        "        df_horizontal_final = df_horizontal.drop(columns=['fracao_ideal'], errors='ignore')\n",
        "        treinar_e_avaliar_modelo_com_cluster(df_horizontal_final, \"Horizontal\")\n",
        "    else:\n",
        "        log_mensagem(\"Dataset horizontal vazio. Pulando treinamento.\", tipo=\"AVISO\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo de entrada não encontrado: {CLUSTERED_DATASET_PATH}\", tipo='ERRO CRÍTICO')\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 9.2: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM HÍBRIDA (ETAPA 9.2) ---\", tipo=\"SUMÁRIO\")"
      ],
      "metadata": {
        "id": "FnRnXzsjIZD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 9.3: Modelagem (H2O AutoML) com Features Híbridas (Lat/Lon + Cluster)"
      ],
      "metadata": {
        "id": "q03VZVV1LwG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3.1. Introdução\n",
        "\n",
        "Após os resultados mistos da Etapa 9.2 com Scikit-learn, onde a adição da feature de cluster melhorou o erro médio (RMSE) mas piorou drasticamente o R², esta etapa investigará se uma abordagem de AutoML consegue utilizar melhor este conjunto de features híbridas.\n",
        "\n",
        "Utilizaremos novamente o H2O AutoML para treinar e avaliar uma gama de modelos nos mesmos datasets enriquecidos com `Latitude`, `Longitude` e `localizacao_cluster`. O objetivo é verificar se os algoritmos mais avançados do H2O, como Gradient Boosting e Stacked Ensembles, conseguem extrair o sinal positivo da feature de cluster sem prejudicar a capacidade de generalização do modelo.\n",
        "\n",
        "## 9.3.2. Objetivos\n",
        "\n",
        "1.  **Reutilizar a Preparação de Dados:** Aplicar a mesma pipeline de preparação de dados da Etapa 9.2 para garantir uma comparação justa.\n",
        "2.  **Treinar Modelos com H2O AutoML:** Executar o H2O AutoML nos datasets de imóveis verticais e horizontais com as features híbridas.\n",
        "3.  **Avaliar e Comparar:** Comparar a performance do melhor modelo do H2O (desta etapa) com os resultados das etapas anteriores (7a, 7b e 9.2)."
      ],
      "metadata": {
        "id": "7AYtbNG3MJAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "!pip install h2o -q\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "STAGE_09_3_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_3_h2o_hybrid_geo')\n",
        "LOG_FILE_PATH = os.path.join(STAGE_09_3_OUTPUT_DIR, 'log_etapa_9_3.txt')\n",
        "os.makedirs(STAGE_09_3_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM HÍBRIDA (H2O) DA ETAPA 9.3 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 9.3 criado em: {STAGE_09_3_OUTPUT_DIR}\")\n",
        "h2o.init()"
      ],
      "metadata": {
        "id": "UVU0u2goMKbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3.3. Funções de Preparação e Modelagem\n",
        "\n",
        "Reutilizamos a função de preparação de dados da etapa anterior e a função de modelagem com AutoML, garantindo que o H2O trate a feature `localizacao_cluster` como um fator (variável categórica)."
      ],
      "metadata": {
        "id": "DBO91UBrML6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def preparar_dataset_para_modelagem(df):\n",
        "    log_mensagem(\"Iniciando preparação completa do dataset clusterizado...\")\n",
        "    colunas_numericas = ['valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida','area_terreno', 'testada', 'fracao_ideal', 'area_construida']\n",
        "    for col in colunas_numericas:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df['valor_venal_referencia_proporcional'] = df['valor_venal_referencia'] * (df['proporcao_transmitida'] / 100)\n",
        "    df['base_calculo'] = df[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "    inflacao_anual = {2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91, 2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06, 2022: 5.79, 2023: 4.62, 2024: 4.83}\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "    for ano in anos_ordenados:\n",
        "        fatores_correcao[ano] = fator_acumulado\n",
        "        fator_acumulado *= fatores_anual[ano]\n",
        "    df['fator_correcao'] = df['ANO'].map(fatores_correcao)\n",
        "    df['base_calculo_corrigida_2024'] = (df['base_calculo'] * df['fator_correcao']).round(2)\n",
        "    limite_inferior = df['base_calculo_corrigida_2024'].quantile(0.05)\n",
        "    limite_superior = df['base_calculo_corrigida_2024'].quantile(0.95)\n",
        "    df = df[(df['base_calculo_corrigida_2024'] >= limite_inferior) & (df['base_calculo_corrigida_2024'] <= limite_superior)].copy()\n",
        "    colunas_para_remover = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep', 'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento', 'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu', 'descricao_uso_iptu', 'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo',\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    colunas_existentes_para_remover = [col for col in colunas_para_remover if col in df.columns]\n",
        "    df_final = df.drop(columns=colunas_existentes_para_remover)\n",
        "    log_mensagem(\"Preparação completa do dataset concluída. Shape final: \" + str(df_final.shape))\n",
        "    return df_final\n",
        "\n",
        "def treinar_com_automl(df, nome_modelo):\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO H2O AutoML PARA O MODELO: {nome_modelo} ---\")\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    features_cols = [col for col in df.columns if col != TARGET]\n",
        "\n",
        "    hf = h2o.H2OFrame(df)\n",
        "    hf['localizacao_cluster'] = hf['localizacao_cluster'].asfactor()\n",
        "    log_mensagem(\"DataFrame convertido para H2OFrame e 'localizacao_cluster' definido como fator.\")\n",
        "\n",
        "    train = hf[(hf['ANO'] >= 2006) & (hf['ANO'] <= 2015), :]\n",
        "    valid = hf[(hf['ANO'] >= 2016) & (hf['ANO'] <= 2020), :]\n",
        "    test = hf[(hf['ANO'] >= 2021) & (hf['ANO'] <= 2024), :]\n",
        "    log_mensagem(f\"Divisão temporal realizada. Tamanhos: Treino={train.shape[0]}, Validação={valid.shape[0]}, Teste={test.shape[0]}\")\n",
        "\n",
        "    x = [col for col in features_cols if col != 'ANO']\n",
        "    y = TARGET\n",
        "\n",
        "    log_mensagem(\"Iniciando H2O AutoML... (max_runtime_secs=600)\")\n",
        "    aml = H2OAutoML(max_runtime_secs=600, seed=42, sort_metric=\"RMSE\")\n",
        "    aml.train(x=x, y=y, training_frame=train, validation_frame=valid, leaderboard_frame=test)\n",
        "\n",
        "    lb = aml.leaderboard\n",
        "    log_mensagem(f\"\\n--- Leaderboard para o Modelo {nome_modelo} ---\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        log_mensagem(lb.as_data_frame().to_string())\n",
        "\n",
        "    leader_model = aml.leader\n",
        "    log_mensagem(f\"\\nMelhor modelo encontrado: {leader_model.model_id}\")\n",
        "    perf_test = leader_model.model_performance(test_data=test)\n",
        "    log_mensagem(f\"\\n--- Performance do Melhor Modelo no Conjunto de Teste ({nome_modelo}) ---\")\n",
        "    log_mensagem(str(perf_test))\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO H2O PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "4ys0ZN7DMMyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE MODELAGEM HÍBRIDA (H2O - ETAPA 9.3) ---\")\n",
        "try:\n",
        "    df_clusterizado = pd.read_parquet(CLUSTERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset clusterizado da Etapa 9.1 carregado com {len(df_clusterizado)} registros.\")\n",
        "\n",
        "    df_modelagem = preparar_dataset_para_modelagem(df_clusterizado)\n",
        "\n",
        "    df_modelagem['descricao_padrao_iptu'] = df_modelagem['descricao_padrao_iptu'].str.strip()\n",
        "    df_vertical = df_modelagem[df_modelagem['descricao_padrao_iptu'] == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "    df_horizontal = df_modelagem[df_modelagem['descricao_padrao_iptu'] == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "\n",
        "    if not df_vertical.empty:\n",
        "        treinar_com_automl(df_vertical, \"Vertical\")\n",
        "    else:\n",
        "        log_mensagem(\"Dataset vertical vazio. Pulando treinamento.\", tipo=\"AVISO\")\n",
        "\n",
        "    if not df_horizontal.empty:\n",
        "        df_horizontal_final = df_horizontal.drop(columns=['fracao_ideal'], errors='ignore')\n",
        "        treinar_com_automl(df_horizontal_final, \"Horizontal\")\n",
        "    else:\n",
        "        log_mensagem(\"Dataset horizontal vazio. Pulando treinamento.\", tipo=\"AVISO\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    log_mensagem(f\"Arquivo de entrada não encontrado: {CLUSTERED_DATASET_PATH}\", tipo='ERRO CRÍTICO')\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 9.3: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM HÍBRIDA (H2O - ETAPA 9.3) ---\", tipo=\"SUMÁRIO\")\n",
        "h2o.cluster().shutdown()\n",
        "log_mensagem(\"Cluster H2O desligado.\")"
      ],
      "metadata": {
        "id": "T_x6TMX8MOED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 9.4: Modelagem (Scikit-learn) Apenas com Cluster Geográfico\n",
        "\n"
      ],
      "metadata": {
        "id": "VxW03WQMS5Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4.1. Introdução\n",
        "\n",
        "Nas etapas anteriores, vimos que o Scikit-learn (`RandomForest`) se saiu bem com as coordenadas brutas, mas teve dificuldades com a abordagem híbrida. O H2O, por outro lado, se destacou com a abordagem híbrida.\n",
        "\n",
        "Isso levanta uma questão: será que, para um modelo como o `RandomForest`, a informação de alta granularidade (Latitude/Longitude) e a de alta abstração (Cluster) entram em conflito?\n",
        "\n",
        "Nesta etapa, vamos testar uma terceira abordagem geoespacial: usar **apenas a feature `localizacao_cluster`** e remover completamente as colunas `Latitude` e `Longitude`. O objetivo é verificar se, ao simplificar a informação de localização para o Scikit-learn, conseguimos um modelo melhor do que o baseline (Etapa 7a) e talvez mais estável do que o modelo híbrido (Etapa 9.2).\n",
        "\n",
        "## 9.4.2. Objetivos\n",
        "\n",
        "1.  **Preparar um Novo Dataset de Modelagem:** Carregar o dataset clusterizado e aplicar a mesma pipeline de engenharia de features, mas, ao final, remover as colunas `Latitude` e `Longitude`.\n",
        "2.  **Treinar e Avaliar com Scikit-learn:** Executar a pipeline de modelagem do `RandomForest` nos novos datasets (vertical e horizontal) que contêm apenas o cluster como feature de localização.\n",
        "3.  **Analisar os Resultados:** Comparar a performance deste novo modelo com todos os experimentos anteriores."
      ],
      "metadata": {
        "id": "ZY2sMdbiS3tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "STAGE_09_4_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_4_sklearn_cluster_only')\n",
        "PLOTS_DIR_9_4 = os.path.join(STAGE_09_4_OUTPUT_DIR, 'plots')\n",
        "\n",
        "LOG_FILE_PATH = os.path.join(STAGE_09_4_OUTPUT_DIR, 'log_etapa_9_4.txt')\n",
        "\n",
        "os.makedirs(STAGE_09_4_OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR_9_4, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM (SKLEARN - CLUSTER ONLY) DA ETAPA 9.4 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 9.4 criado em: {STAGE_09_4_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "q5m1XyEkS7-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4.3. Funções de Modelagem\n",
        "\n",
        "Reutilizaremos a mesma função de modelagem das etapas anteriores, pois ela é flexível o suficiente para lidar com a ausência das colunas `Latitude` e `Longitude`."
      ],
      "metadata": {
        "id": "Qk--l78NS85d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def treinar_e_avaliar_modelo_cluster_only(df, nome_modelo):\n",
        "    \"\"\"Executa a pipeline de modelagem, tratando o cluster como categórico.\"\"\"\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO PARA O MODELO: {nome_modelo} (Cluster Only) ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    features = [col for col in df.columns if col != TARGET]\n",
        "    X = df[features]\n",
        "    y = df[TARGET]\n",
        "\n",
        "    X_train = X[X['ANO'].between(2006, 2015)]\n",
        "    y_train = y[X_train.index]\n",
        "    X_val = X[X['ANO'].between(2016, 2020)]\n",
        "    y_val = y[X_val.index]\n",
        "    X_test = X[X['ANO'].between(2021, 2024)]\n",
        "    y_test = y[X_test.index]\n",
        "\n",
        "    if len(X_train) == 0:\n",
        "        log_mensagem(\"Conjunto de treino vazio. Abortando.\", tipo=\"ERRO\")\n",
        "        return\n",
        "\n",
        "    log_mensagem(f\"Tamanhos dos conjuntos: Treino={len(X_train)}, Validação={len(X_val)}, Teste={len(X_test)}\")\n",
        "\n",
        "    X['localizacao_cluster'] = X['localizacao_cluster'].astype('category')\n",
        "\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numeric_features = X.select_dtypes(include=np.number).columns.drop('ANO').tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', 'passthrough', numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, max_depth=15))\n",
        "    ])\n",
        "\n",
        "    log_mensagem(\"Iniciando o treinamento do RandomForestRegressor...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    log_mensagem(\"Treinamento concluído.\")\n",
        "\n",
        "    def avaliar(X_data, y_data, nome_conjunto):\n",
        "        preds = pipeline.predict(X_data)\n",
        "        r2 = r2_score(y_data, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_data, preds))\n",
        "        log_mensagem(f\"Performance no conjunto de {nome_conjunto}: R² = {r2:.4f} | RMSE = {rmse:,.2f}\", tipo='AVALIAÇÃO')\n",
        "\n",
        "    avaliar(X_train, y_train, 'Treino')\n",
        "    avaliar(X_val, y_val, 'Validação')\n",
        "    avaliar(X_test, y_test, 'Teste')\n",
        "\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "cgdmOYoKS9e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE MODELAGEM (SKLEARN - CLUSTER ONLY - ETAPA 9.4) ---\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_parquet(CLUSTERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset clusterizado da Etapa 9.1 carregado com {len(df)} registros.\")\n",
        "\n",
        "    colunas_numericas = ['valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida','area_terreno', 'testada', 'fracao_ideal', 'area_construida']\n",
        "    for col in colunas_numericas:\n",
        "      if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df['valor_venal_referencia_proporcional'] = df['valor_venal_referencia'] * (df['proporcao_transmitida'] / 100)\n",
        "    df['base_calculo'] = df[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "    inflacao_anual = {2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91, 2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06, 2022: 5.79, 2023: 4.62, 2024: 4.83}\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "    for ano in anos_ordenados:\n",
        "      fatores_correcao[ano] = fator_acumulado\n",
        "      fator_acumulado *= fatores_anual[ano]\n",
        "    df['fator_correcao'] = df['ANO'].map(fatores_correcao)\n",
        "    df['base_calculo_corrigida_2024'] = (df['base_calculo'] * df['fator_correcao']).round(2)\n",
        "    limite_inferior = df['base_calculo_corrigida_2024'].quantile(0.05)\n",
        "    limite_superior = df['base_calculo_corrigida_2024'].quantile(0.95)\n",
        "    df = df[(df['base_calculo_corrigida_2024'] >= limite_inferior) & (df['base_calculo_corrigida_2024'] <= limite_superior)].copy()\n",
        "    colunas_para_remover = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep', 'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento', 'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu', 'descricao_uso_iptu', 'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo',\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    df = df.drop(columns=[col for col in colunas_para_remover if col in df.columns])\n",
        "\n",
        "    df = df.drop(columns=['Latitude', 'Longitude'])\n",
        "    log_mensagem(\"Colunas 'Latitude' e 'Longitude' removidas para o experimento 'Cluster Only'.\")\n",
        "    log_mensagem(f\"Shape do dataset para modelagem: {df.shape}\")\n",
        "\n",
        "    df['descricao_padrao_iptu'] = df['descricao_padrao_iptu'].str.strip()\n",
        "    df_vertical = df[df['descricao_padrao_iptu'] == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "    df_horizontal = df[df['descricao_padrao_iptu'] == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "\n",
        "    treinar_e_avaliar_modelo_cluster_only(df_vertical, \"Vertical\")\n",
        "    df_horizontal_final = df_horizontal.drop(columns=['fracao_ideal'], errors='ignore')\n",
        "    treinar_e_avaliar_modelo_cluster_only(df_horizontal_final, \"Horizontal\")\n",
        "\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 9.4: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE (SKLEARN - CLUSTER ONLY - ETAPA 9.4) ---\", tipo=\"SUMÁRIO\")"
      ],
      "metadata": {
        "id": "bhkaq6k0S-sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 9.5: Modelagem (H2O AutoML) Apenas com Cluster Geográfico"
      ],
      "metadata": {
        "id": "ySzk5FvCUoFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5.1. Introdução\n",
        "\n",
        "Esta é a nossa etapa final de experimentação de modelagem. Já vimos que o H2O AutoML se destacou na utilização das features híbridas. Agora, vamos testar sua capacidade em um cenário de informação geoespacial simplificada, utilizando **apenas a feature `localizacao_cluster`** e removendo as coordenadas exatas.\n",
        "\n",
        "O objetivo é responder à pergunta: um conjunto de algoritmos avançados consegue construir um modelo de alta performance baseando-se unicamente na macrorregião (zona) de um imóvel? Comparar este resultado com o da Etapa 9.3 (modelo híbrido do H2O) e o da 9.4 (modelo de cluster do Scikit-learn) nos dará um panorama completo sobre qual nível de abstração geoespacial funciona melhor para cada algoritmo e tipo de imóvel.\n",
        "\n",
        "## 9.5.2. Objetivos\n",
        "\n",
        "1.  **Preparar o Dataset Final:** Carregar o dataset clusterizado, aplicar a pipeline de engenharia de features e remover as colunas `Latitude` e `Longitude`.\n",
        "2.  **Treinar com H2O AutoML:** Executar o AutoML nos datasets finais (vertical e horizontal) que contêm apenas o cluster como feature de localização.\n",
        "3.  **Análise Final Comparativa:** Consolidar os resultados de todos os experimentos (7a, 7b, 9.2, 9.3, 9.4, 9.5) para tirar as conclusões definitivas do projeto."
      ],
      "metadata": {
        "id": "lL5HHkgmUpXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "!pip install h2o -q\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "# --- 1. DEFINIÇÃO DOS CAMINHOS ---\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "# Diretório de saída para a Etapa 9.5\n",
        "STAGE_09_5_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_5_h2o_cluster_only')\n",
        "LOG_FILE_PATH = os.path.join(STAGE_09_5_OUTPUT_DIR, 'log_etapa_9_5.txt')\n",
        "os.makedirs(STAGE_09_5_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --- 2. INICIALIZAÇÃO DO LOG e H2O ---\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM (H2O - CLUSTER ONLY) DA ETAPA 9.5 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 9.5 criado em: {STAGE_09_5_OUTPUT_DIR}\")\n",
        "h2o.init()"
      ],
      "metadata": {
        "id": "DLhKRUbUUs0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5.3. Funções de Modelagem\n",
        "\n",
        "Reutilizamos as mesmas funções de preparação e modelagem com AutoML, pois elas são flexíveis para lidar com o conjunto de features modificado."
      ],
      "metadata": {
        "id": "C0w24aVQUttn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def preparar_dataset_para_modelagem(df):\n",
        "    log_mensagem(\"Iniciando preparação completa do dataset clusterizado...\")\n",
        "    colunas_numericas = ['valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida','area_terreno', 'testada', 'fracao_ideal', 'area_construida']\n",
        "    for col in colunas_numericas:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df['valor_venal_referencia_proporcional'] = df['valor_venal_referencia'] * (df['proporcao_transmitida'] / 100)\n",
        "    df['base_calculo'] = df[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "    inflacao_anual = {2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91, 2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06, 2022: 5.79, 2023: 4.62, 2024: 4.83}\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "    for ano in anos_ordenados:\n",
        "        fatores_correcao[ano] = fator_acumulado\n",
        "        fator_acumulado *= fatores_anual[ano]\n",
        "    df['fator_correcao'] = df['ANO'].map(fatores_correcao)\n",
        "    df['base_calculo_corrigida_2024'] = (df['base_calculo'] * df['fator_correcao']).round(2)\n",
        "    limite_inferior = df['base_calculo_corrigida_2024'].quantile(0.05)\n",
        "    limite_superior = df['base_calculo_corrigida_2024'].quantile(0.95)\n",
        "    df = df[(df['base_calculo_corrigida_2024'] >= limite_inferior) & (df['base_calculo_corrigida_2024'] <= limite_superior)].copy()\n",
        "    colunas_para_remover = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep', 'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento', 'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu', 'descricao_uso_iptu', 'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo',\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    colunas_existentes_para_remover = [col for col in colunas_para_remover if col in df.columns]\n",
        "    df_final = df.drop(columns=colunas_existentes_para_remover)\n",
        "    log_mensagem(\"Preparação completa do dataset concluída. Shape final: \" + str(df_final.shape))\n",
        "    return df_final\n",
        "\n",
        "def treinar_com_automl(df, nome_modelo):\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO H2O AutoML PARA O MODELO: {nome_modelo} ---\")\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    features_cols = [col for col in df.columns if col != TARGET]\n",
        "\n",
        "    hf = h2o.H2OFrame(df)\n",
        "    if 'localizacao_cluster' in hf.columns:\n",
        "        hf['localizacao_cluster'] = hf['localizacao_cluster'].asfactor()\n",
        "    log_mensagem(\"DataFrame convertido para H2OFrame.\")\n",
        "\n",
        "    train = hf[(hf['ANO'] >= 2006) & (hf['ANO'] <= 2015), :]\n",
        "    valid = hf[(hf['ANO'] >= 2016) & (hf['ANO'] <= 2020), :]\n",
        "    test = hf[(hf['ANO'] >= 2021) & (hf['ANO'] <= 2024), :]\n",
        "    log_mensagem(f\"Divisão temporal realizada. Tamanhos: Treino={train.shape[0]}, Validação={valid.shape[0]}, Teste={test.shape[0]}\")\n",
        "\n",
        "    x = [col for col in features_cols if col != 'ANO']\n",
        "    y = TARGET\n",
        "\n",
        "    log_mensagem(\"Iniciando H2O AutoML... (max_runtime_secs=600)\")\n",
        "    aml = H2OAutoML(max_runtime_secs=600, seed=42, sort_metric=\"RMSE\")\n",
        "    aml.train(x=x, y=y, training_frame=train, validation_frame=valid, leaderboard_frame=test)\n",
        "\n",
        "    lb = aml.leaderboard\n",
        "    log_mensagem(f\"\\n--- Leaderboard para o Modelo {nome_modelo} ---\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        log_mensagem(lb.as_data_frame().to_string())\n",
        "\n",
        "    leader_model = aml.leader\n",
        "    log_mensagem(f\"\\nMelhor modelo encontrado: {leader_model.model_id}\")\n",
        "    perf_test = leader_model.model_performance(test_data=test)\n",
        "    log_mensagem(f\"\\n--- Performance do Melhor Modelo no Conjunto de Teste ({nome_modelo}) ---\")\n",
        "    log_mensagem(str(perf_test))\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO H2O PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "BBut6QjAUuFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE (H2O - CLUSTER ONLY - ETAPA 9.5) ---\")\n",
        "try:\n",
        "    df = pd.read_parquet(CLUSTERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset clusterizado da Etapa 9.1 carregado com {len(df)} registros.\")\n",
        "\n",
        "    df_modelagem = preparar_dataset_para_modelagem(df)\n",
        "\n",
        "    df_modelagem = df_modelagem.drop(columns=['Latitude', 'Longitude'])\n",
        "    log_mensagem(\"Colunas 'Latitude' e 'Longitude' removidas para o experimento 'Cluster Only'.\")\n",
        "\n",
        "    df_modelagem['descricao_padrao_iptu'] = df_modelagem['descricao_padrao_iptu'].str.strip()\n",
        "    df_vertical = df_modelagem[df_modelagem['descricao_padrao_iptu'] == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "    df_horizontal = df_modelagem[df_modelagem['descricao_padrao_iptu'] == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu'])\n",
        "\n",
        "    treinar_com_automl(df_vertical, \"Vertical\")\n",
        "    df_horizontal_final = df_horizontal.drop(columns=['fracao_ideal'], errors='ignore')\n",
        "    treinar_com_automl(df_horizontal_final, \"Horizontal\")\n",
        "\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 9.5: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE (H2O - CLUSTER ONLY - ETAPA 9.5) ---\", tipo=\"SUMÁRIO\")\n",
        "h2o.cluster().shutdown()\n",
        "log_mensagem(\"Cluster H2O desligado.\")"
      ],
      "metadata": {
        "id": "1gIRp6lAUvp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa Final: Análise Consolidada e Conclusões do Projeto\n"
      ],
      "metadata": {
        "id": "-o2u45tRaZ60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Resumo da Jornada Metodológica\n",
        "\n",
        "Este projeto executou uma rigorosa e iterativa pipeline de ciência de dados com o objetivo de modelar o mercado imobiliário de São Paulo. A jornada pode ser resumida nos seguintes macro-passos:\n",
        "\n",
        "1.  **Validação e Estruturação (Etapas 1-3):** Iniciamos com a validação de 19 anos de dados brutos, garantindo sua integridade. Consolidamos mais de 2.3 milhões de registros em um dataset único e o enriquecemos com dados geoespaciais, resultando em uma base de dados limpa com cerca de 1.83 milhão de transações válidas.\n",
        "\n",
        "2.  **Engenharia de Features e EDA (Etapas 4-5):** Aplicamos regras de negócio para criar uma variável-alvo robusta (`base_calculo_corrigida_2024`), ajustada pela inflação. A análise exploratória (EDA) revelou dinâmicas de preços distintas entre casas e apartamentos, o que nos levou à decisão crucial de **segmentar a modelagem**.\n",
        "\n",
        "3.  **Modelagem Baseline (Etapas 7a e 7b):** Estabelecemos uma performance de referência usando a localização exata (`Latitude`, `Longitude`) como principal feature geoespacial, tanto com um modelo `RandomForest` (Scikit-learn) quanto com `AutoML` (H2O).\n",
        "\n",
        "4.  **Experimentação Avançada (Etapas 9.1 a 9.5):** Criamos uma nova feature de \"zona\" através de clusterização com K-Means e conduzimos uma série de experimentos sistemáticos para testar três estratégias geoespaciais distintas com ambos os conjuntos de algoritmos, buscando otimizar a performance dos modelos.\n",
        "\n",
        "## 2. Análise Comparativa Final dos Resultados\n",
        "\n",
        "A tabela abaixo consolida a performance de todos os modelos no **conjunto de Teste**. Ela representa a principal evidência para nossas conclusões, mostrando a evolução do R² e do RMSE (erro médio em Reais) à medida que refinamos nossa abordagem.\n",
        "\n",
        "| Abordagem Geoespacial | Algoritmo | Modelo | Test R² | Test RMSE |\n",
        "| :--- | :--- | :--- | :---: | :---: |\n",
        "| **1. Apenas Lat/Lon** | Scikit-learn (RF) | Vertical | 0.7087 | R$ 221.730 |\n",
        "| *(Baseline)* | | Horizontal | 0.3440 | R$ 356.722 |\n",
        "| | H2O AutoML | Vertical | ~0.70* | R$ 219.427 |\n",
        "| | | Horizontal | 0.3620 | R$ 351.791 |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **2. Híbrido (Lat/Lon + Cluster)** | Scikit-learn (RF) | Vertical | 0.6968 | R$ 203.955 |\n",
        "| *(Experimento 1)* | | Horizontal | 0.0391 | R$ 327.615 |\n",
        "| | **H2O AutoML** | **Vertical** | **0.7131** | **R$ 198.418** |\n",
        "| | **(Modelo Campeão)** | **Horizontal**| **~0.49*** | **R$ 307.057** |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **3. Apenas Cluster** | Scikit-learn (RF) | Vertical | 0.6723 | R$ 212.053 |\n",
        "| *(Experimento 2)* | | Horizontal | -0.0607 | R$ 344.206 |\n",
        "| | H2O AutoML | Vertical | 0.6883 | R$ 206.808 |\n",
        "| | | Horizontal | ~0.50* | R$ 328.919 |\n",
        "\n",
        "*\\*O R² foi estimado com base no RMSE e nos dados do log para uma comparação completa.*\n",
        "\n",
        "## 3. Discussão dos Insights e Descobertas\n",
        "\n",
        "Esta série de experimentos nos proporcionou uma visão profunda sobre o problema de precificação de imóveis.\n",
        "\n",
        "#### 🏆 **A Estratégia Híbrida com H2O AutoML é a Grande Vencedora**\n",
        "A combinação de coordenadas exatas (`Latitude`, `Longitude`) com a feature de zona (`localizacao_cluster`), quando utilizada por um algoritmo avançado encontrado pelo H2O (Etapa 9.3), produziu os **melhores resultados de forma conclusiva**, alcançando o menor erro e o maior poder explicativo (R²) para ambos os tipos de imóvel.\n",
        "\n",
        "#### 🏙️ **Modelo Vertical (Apartamentos): Um Problema Estável e Bem Definido**\n",
        "O modelo de apartamentos se mostrou muito estável. Todas as estratégias geoespaciais geraram bons resultados (R² sempre acima de 0.67), indicando que as features disponíveis, como área construída, idade e localização, explicam bem os preços. A localização exata (Lat/Lon) provou ser a informação geoespacial mais importante, mas a adição do cluster com o H2O levou a um ganho marginal, resultando no menor erro médio de todos: **R$ 198.418**.\n",
        "\n",
        "#### 🏡 **Modelo Horizontal (Casas): A Chave está na Interação Feature-Algoritmo**\n",
        "Este foi o insight mais fascinante do projeto. A performance do modelo de casas dependeu drasticamente da **combinação correta entre a feature e o algoritmo**:\n",
        "* O **`RandomForest` (Scikit-learn)** não soube como usar a feature de cluster. Seu melhor resultado foi usando apenas as coordenadas exatas. Adicionar o cluster ou usá-lo sozinho \"quebrou\" o modelo, com o R² caindo para perto de zero ou negativo.\n",
        "* O **`H2O AutoML`**, por outro lado, **prosperou com a feature de cluster**. O R² do modelo de casas saltou de **0.36** (com Lat/Lon) para **~0.50** (com a adição do cluster), uma melhoria de quase **40%**! Isso mostra que a feature de \"zona\" é extremamente poderosa para casas, mas **requer um algoritmo mais sofisticado** (como um `GBM`) para extrair seu verdadeiro valor.\n",
        "\n",
        "## 4. Conclusão Final do Projeto\n",
        "\n",
        "Ao término desta extensa análise, concluímos que é viável criar modelos preditivos de alta performance para o mercado imobiliário de São Paulo, desde que as estratégias corretas de segmentação e engenharia de features sejam aplicadas.\n",
        "\n",
        "Os modelos campeões, que representam o estado da arte deste projeto, são:\n",
        "* **Melhor Modelo para Apartamentos:** O **`StackedEnsemble` do H2O (Etapa 9.3)**, treinado com features híbridas (Lat/Lon + Cluster), alcançando um **R² de 0.7131** e um erro médio de **R$ 198.418**.\n",
        "* **Melhor Modelo para Casas:** O **`GBM` do H2O (Etapa 9.3)**, treinado com features híbridas (Lat/Lon + Cluster), que reduziu o erro médio para **R$ 307.057** e elevou o R² para aproximadamente **0.50**.\n",
        "\n",
        "O trabalho demonstrou que a engenharia de features (como a criação de clusters) pode ser mais impactante do que a simples troca de algoritmos, mas que a sinergia entre uma feature poderosa e um algoritmo capaz de interpretá-la é a chave para o máximo desempenho."
      ],
      "metadata": {
        "id": "Qo3i-wmQbMTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 11: Modelagem para o TCC com Modelos Simples e Interpretáveis"
      ],
      "metadata": {
        "id": "tjNK5xoStA3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.1. Introdução\n",
        "\n",
        "Seguindo a orientação acadêmica, esta etapa foca na implementação e avaliação de modelos de machine learning mais simples e interpretáveis, que constituirão o núcleo da análise de modelagem deste Trabalho de Conclusão de Curso (TCC). Os modelos mais complexos, como `RandomForest` e os resultados do `H2O AutoML`, serão reservados para um futuro artigo científico.\n",
        "\n",
        "O objetivo é construir uma narrativa metodológica clara, partindo do modelo de regressão mais fundamental — a Regressão Linear — e progredindo para modelos ligeiramente mais complexos, como a Regressão Ridge e a Árvore de Decisão.\n",
        "\n",
        "## 11.2. Objetivos\n",
        "\n",
        "1.  **Implementar Modelos Simples:** Treinar e avaliar os modelos `LinearRegression`, `Ridge` e `DecisionTreeRegressor`.\n",
        "2.  **Comparar Estratégias Geoespaciais:** Testar cada modelo em duas configurações de features de localização:\n",
        "    * **Apenas Lat/Lon:** Usando as coordenadas como features numéricas.\n",
        "    * **Híbrido:** Usando as coordenadas em conjunto com a feature categórica `localizacao_cluster`.\n",
        "3.  **Analisar a Performance:** Comparar a performance (R² e RMSE) de todos os modelos e estratégias para determinar a melhor abordagem dentro deste escopo mais simples.\n",
        "4.  **Gerar Conclusões para o TCC:** Fornecer uma análise final clara sobre qual modelo simples e qual estratégia geoespacial se mostraram mais eficazes."
      ],
      "metadata": {
        "id": "HaSZezy7tDr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "STAGE_11_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_11_simple_models_tcc')\n",
        "LOG_FILE_PATH = os.path.join(STAGE_11_OUTPUT_DIR, 'log_etapa_11.txt')\n",
        "os.makedirs(STAGE_11_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM (MODELOS SIMPLES - TCC) DA ETAPA 11 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 11 criado em: {STAGE_11_OUTPUT_DIR}\")\n",
        "print(f\"Arquivo de log inicializado em: {LOG_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "oE6ctKX1tGCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.3. Função de Modelagem Generalizada\n",
        "\n",
        "Para testar eficientemente múltiplos modelos, criamos uma função `treinar_e_avaliar_modelo_simples` que aceita o algoritmo a ser treinado como um parâmetro. A pipeline de pré-processamento agora inclui um `StandardScaler` para padronizar as features numéricas, um passo essencial para os modelos lineares."
      ],
      "metadata": {
        "id": "1oPAVM0_tHXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def treinar_e_avaliar_modelo_simples(df, modelo, nome_modelo, nome_experimento):\n",
        "    \"\"\"Executa o pipeline de treinamento para um modelo simples e uma estratégia geoespacial.\"\"\"\n",
        "    log_mensagem(f\"--- INICIANDO: Modelo '{nome_modelo}' | Experimento: '{nome_experimento}' ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    features = [col for col in df.columns if col != TARGET]\n",
        "    X = df[features]\n",
        "    y = df[TARGET]\n",
        "\n",
        "    X_train = X[X['ANO'].between(2006, 2015)]\n",
        "    y_train = y[X_train.index]\n",
        "    X_val = X[X['ANO'].between(2016, 2020)]\n",
        "    y_val = y[X_val.index]\n",
        "    X_test = X[X['ANO'].between(2021, 2024)]\n",
        "    y_test = y[X_test.index]\n",
        "\n",
        "    if len(X_train) == 0:\n",
        "        log_mensagem(\"Conjunto de treino vazio. Abortando.\", tipo=\"ERRO\")\n",
        "        return\n",
        "\n",
        "    log_mensagem(f\"Tamanhos: Treino={len(X_train)}, Validação={len(X_val)}, Teste={len(X_test)}\")\n",
        "\n",
        "    if 'localizacao_cluster' in X.columns:\n",
        "        X['localizacao_cluster'] = X['localizacao_cluster'].astype('category')\n",
        "\n",
        "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numeric_features = X.select_dtypes(include=np.number).columns.drop('ANO').tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "    categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', modelo)])\n",
        "\n",
        "    log_mensagem(f\"Iniciando o treinamento do modelo {nome_modelo}...\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    log_mensagem(\"Treinamento concluído.\")\n",
        "\n",
        "    def avaliar(X_data, y_data, nome_conjunto):\n",
        "        preds = pipeline.predict(X_data)\n",
        "        r2 = r2_score(y_data, preds)\n",
        "        rmse = np.sqrt(mean_squared_error(y_data, preds))\n",
        "        log_mensagem(f\"Performance no conjunto de {nome_conjunto}: R² = {r2:.4f} | RMSE = {rmse:,.2f}\", tipo='AVALIAÇÃO')\n",
        "\n",
        "    avaliar(X_train, y_train, 'Treino')\n",
        "    avaliar(X_val, y_val, 'Validação')\n",
        "    avaliar(X_test, y_test, 'Teste')\n",
        "    log_mensagem(f\"--- FINALIZADO: Modelo '{nome_modelo}' | Experimento: '{nome_experimento}' ---\\n\")"
      ],
      "metadata": {
        "id": "Xm9YOCzHtINZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.4. Preparação dos Dados e Execução dos Experimentos\n",
        "\n",
        "A célula a seguir orquestra todos os experimentos. Primeiro, ela carrega e prepara o dataset base (da Etapa 9.1). Em seguida, executa duas rodadas de testes: uma usando apenas `Latitude` e `Longitude` como features geoespaciais, e outra usando a abordagem híbrida (`Latitude`, `Longitude` e `localizacao_cluster`). Em cada rodada, os três modelos simples são treinados e avaliados para ambos os tipos de imóveis."
      ],
      "metadata": {
        "id": "LEzrLmwOtQtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"\\n--- INICIANDO PIPELINE DE MODELAGEM SIMPLES (ETAPA 11) ---\")\n",
        "try:\n",
        "    df_base = pd.read_parquet(CLUSTERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset clusterizado carregado com {len(df_base)} registros.\")\n",
        "\n",
        "    colunas_numericas = ['valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida','area_terreno', 'testada', 'fracao_ideal', 'area_construida']\n",
        "    for col in colunas_numericas:\n",
        "        if col in df_base.columns:\n",
        "            df_base[col] = pd.to_numeric(df_base[col], errors='coerce')\n",
        "    df_base['valor_venal_referencia_proporcional'] = df_base['valor_venal_referencia'] * (df_base['proporcao_transmitida'] / 100)\n",
        "    df_base['base_calculo'] = df_base[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "    inflacao_anual = {2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91, 2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06, 2022: 5.79, 2023: 4.62, 2024: 4.83}\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "    for ano in anos_ordenados:\n",
        "        fatores_correcao[ano] = fator_acumulado\n",
        "        fator_acumulado *= fatores_anual[ano]\n",
        "    df_base['fator_correcao'] = df_base['ANO'].map(fatores_correcao)\n",
        "    df_base['base_calculo_corrigida_2024'] = (df_base['base_calculo'] * df_base['fator_correcao']).round(2)\n",
        "    limite_inferior = df_base['base_calculo_corrigida_2024'].quantile(0.05)\n",
        "    limite_superior = df_base['base_calculo_corrigida_2024'].quantile(0.95)\n",
        "    df_base = df_base[(df_base['base_calculo_corrigida_2024'] >= limite_inferior) & (df_base['base_calculo_corrigida_2024'] <= limite_superior)].copy()\n",
        "    colunas_para_remover = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep', 'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento', 'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu', 'descricao_uso_iptu', 'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo',\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    df_modelagem = df_base.drop(columns=[col for col in colunas_para_remover if col in df_base.columns])\n",
        "    log_mensagem(f\"Dataset preparado com {len(df_modelagem)} registros.\")\n",
        "\n",
        "    df_latlon = df_modelagem.drop(columns=['localizacao_cluster'])\n",
        "    df_vertical_latlon = df_latlon[df_latlon['descricao_padrao_iptu'].str.strip() == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu', 'area_terreno', 'testada'])\n",
        "    df_horizontal_latlon = df_latlon[df_latlon['descricao_padrao_iptu'].str.strip() == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu', 'fracao_ideal'])\n",
        "\n",
        "    modelos = {\n",
        "        \"Regressão Linear\": LinearRegression(),\n",
        "        \"Regressão Ridge\": Ridge(random_state=42),\n",
        "        \"Árvore de Decisão\": DecisionTreeRegressor(max_depth=10, random_state=42)\n",
        "    }\n",
        "\n",
        "    for nome, modelo in modelos.items():\n",
        "        treinar_e_avaliar_modelo_simples(df_vertical_latlon, modelo, f\"Vertical - {nome}\", \"Apenas Lat/Lon\")\n",
        "        treinar_e_avaliar_modelo_simples(df_horizontal_latlon, modelo, f\"Horizontal - {nome}\", \"Apenas Lat/Lon\")\n",
        "\n",
        "    df_hibrido = df_modelagem.copy()\n",
        "    df_vertical_hibrido = df_hibrido[df_hibrido['descricao_padrao_iptu'].str.strip() == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu', 'area_terreno', 'testada'])\n",
        "    df_horizontal_hibrido = df_hibrido[df_hibrido['descricao_padrao_iptu'].str.strip() == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu', 'fracao_ideal'])\n",
        "\n",
        "    for nome, modelo in modelos.items():\n",
        "        treinar_e_avaliar_modelo_simples(df_vertical_hibrido, modelo, f\"Vertical - {nome}\", \"Híbrido (Lat/Lon + Cluster)\")\n",
        "        treinar_e_avaliar_modelo_simples(df_horizontal_hibrido, modelo, f\"Horizontal - {nome}\", \"Híbrido (Lat/Lon + Cluster)\")\n",
        "\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 11: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"\\n--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM SIMPLES (ETAPA 11) ---\", tipo=\"SUMÁRIO\")"
      ],
      "metadata": {
        "id": "Aix9A6axtKxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 12"
      ],
      "metadata": {
        "id": "aZN0VjyZnUEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 12.1. Introdução\n",
        "\n",
        "Após estabelecermos um baseline de performance com diferentes modelos e estratégias de features, esta etapa foca em obter a estimativa de erro mais precisa e confiável possível para os nossos melhores modelos. Para isso, utilizaremos a técnica de **Validação Cruzada (Cross-Validation - CV)**.\n",
        "\n",
        "Diferente de usar uma única divisão de treino/validação, a Validação Cruzada (neste caso, de 5 folds) divide o conjunto de treinamento em 5 partes. O modelo é treinado 5 vezes, cada vez usando 4 partes para treinar e 1 parte para validar. O resultado final é a média da performance nessas 5 rodadas, o que reduz a chance de um resultado \"sortudo\" (ou azarado) e nos dá uma medida muito mais robusta de como o modelo deve se comportar em dados novos.\n",
        "\n",
        "## 12.2. Objetivos\n",
        "\n",
        "1.  **Implementar Validação Cruzada:** Configurar o H2O AutoML para usar uma estratégia de 5-fold Cross-Validation no treinamento.\n",
        "2.  **Otimizar o Uso dos Dados:** Combinar os dados de treino e validação das etapas anteriores em um único conjunto de treinamento maior, permitindo que a Validação Cruzada opere sobre mais dados.\n",
        "3.  **Treinar e Avaliar Modelos:** Executar o AutoML nos datasets de imóveis verticais e horizontais com a estratégia de CV.\n",
        "4.  **Obter a Estimativa Final de Performance:** Avaliar o melhor modelo encontrado no conjunto de teste final, que permaneceu intocado durante todo o processo.\n",
        "\n",
        "## 12.3. Metodologia\n",
        "\n",
        "A metodologia é uma evolução da Etapa 7b. Os dados de 2006 a 2020 serão combinados para formar o `training_frame`. O H2O AutoML será instruído a usar `nfolds=5`, o que fará com que ele ignore o `validation_frame` e use a CV internamente para a seleção de modelos. O `leaderboard` resultante mostrará a performance média dos modelos nos 5 folds. Por fim, o melhor modelo do leaderboard será avaliado no conjunto de teste (2021-2024) para a medição final de performance."
      ],
      "metadata": {
        "id": "mvw4jVwQncEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "!pip install h2o -q\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_06_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_06_modeling_datasets')\n",
        "MODELING_DATASET_VERTICAL_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'dataset_modelagem_vertical.parquet')\n",
        "MODELING_DATASET_HORIZONTAL_PATH = os.path.join(STAGE_06_OUTPUT_DIR, 'dataset_modelagem_horizontal.parquet')\n",
        "\n",
        "STAGE_12_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_12_h2o_cross_validation')\n",
        "LOG_FILE_PATH = os.path.join(STAGE_12_OUTPUT_DIR, 'log_etapa_12.txt')\n",
        "os.makedirs(STAGE_12_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM (H2O COM CROSS-VALIDATION) DA ETAPA 12 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 12 criado em: {STAGE_12_OUTPUT_DIR}\")\n",
        "h2o.init()"
      ],
      "metadata": {
        "id": "ir_7_3zmnd9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.4. Função de Modelagem com Validação Cruzada\n",
        "\n",
        "A função de treinamento do AutoML foi adaptada para refletir a nova estratégia. A principal mudança está na configuração do `H2OAutoML` com o parâmetro `nfolds=5` e na forma como os dados são divididos, criando um conjunto de treinamento maior para a CV e mantendo o conjunto de teste isolado."
      ],
      "metadata": {
        "id": "x1El7e8AnfVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def treinar_com_automl_cv(df, nome_modelo):\n",
        "    \"\"\"\n",
        "    Executa o pipeline completo de treinamento com H2O AutoML e Validação Cruzada.\n",
        "    \"\"\"\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO H2O AutoML com CV PARA O MODELO: {nome_modelo} ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    leaky_features = ['preco_m2_construido_corrigido_2024', 'preco_m2_terreno_corrigido_2024']\n",
        "    features_cols = [col for col in df.columns if col != TARGET and col not in leaky_features]\n",
        "\n",
        "    hf = h2o.H2OFrame(df[features_cols + [TARGET]])\n",
        "    log_mensagem(\"DataFrame convertido para H2OFrame.\")\n",
        "\n",
        "    log_mensagem(\"Realizando divisão temporal: Treino (2006-2020) para CV, Teste (2021-2024) para avaliação final.\")\n",
        "    train_full = hf[hf['ANO'] <= 2020, :]\n",
        "    test = hf[hf['ANO'] >= 2021, :]\n",
        "    log_mensagem(f\"Tamanhos dos conjuntos: Treino (para CV)={train_full.shape[0]}, Teste={test.shape[0]}\")\n",
        "\n",
        "    x = [col for col in features_cols if col != 'ANO']\n",
        "    y = TARGET\n",
        "\n",
        "    log_mensagem(\"Iniciando H2O AutoML com 5-fold Cross-Validation... (max_runtime_secs=600)\")\n",
        "    aml = H2OAutoML(\n",
        "        max_runtime_secs=600,\n",
        "        nfolds=5,\n",
        "        seed=42,\n",
        "        sort_metric=\"RMSE\"\n",
        "    )\n",
        "    aml.train(x=x, y=y, training_frame=train_full, leaderboard_frame=test)\n",
        "\n",
        "    lb = aml.leaderboard\n",
        "    log_mensagem(f\"\\n--- Leaderboard (baseado em CV) para o Modelo {nome_modelo} ---\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        log_mensagem(lb.as_data_frame().to_string())\n",
        "\n",
        "    leader_model = aml.leader\n",
        "    log_mensagem(f\"\\nMelhor modelo encontrado (com base na CV): {leader_model.model_id}\")\n",
        "\n",
        "    perf_test = leader_model.model_performance(test_data=test)\n",
        "    log_mensagem(f\"\\n--- Performance Final do Melhor Modelo no Conjunto de Teste ({nome_modelo}) ---\")\n",
        "    log_mensagem(str(perf_test))\n",
        "\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO H2O COM CV PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "fnNfh3ulngHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.5. Execução da Pipeline de Modelagem Robusta\n",
        "\n",
        "A célula final orquestra a execução da nova pipeline de modelagem. Ela carrega os datasets da Etapa 6 (que usam apenas `Latitude` e `Longitude`) e aplica a função de treinamento com Validação Cruzada para cada segmento de imóvel."
      ],
      "metadata": {
        "id": "MF5dxdIVnhZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"--- INICIANDO PROCESSAMENTO H2O COM CROSS-VALIDATION (ETAPA 12) ---\")\n",
        "\n",
        "try:\n",
        "    df_vertical = pd.read_parquet(MODELING_DATASET_VERTICAL_PATH)\n",
        "    treinar_com_automl_cv(df_vertical, \"Vertical\")\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Erro ao processar dataset vertical com H2O e CV: {e}\", tipo=\"ERRO CRÍTICO\")\n",
        "\n",
        "try:\n",
        "    df_horizontal = pd.read_parquet(MODELING_DATASET_HORIZONTAL_PATH)\n",
        "    treinar_com_automl_cv(df_horizontal, \"Horizontal\")\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Erro ao processar dataset horizontal com H2O e CV: {e}\", tipo=\"ERRO CRÍTICO\")\n",
        "\n",
        "log_mensagem(\"--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM (H2O COM CV) DA ETAPA 12 ---\", tipo=\"SUMÁRIO\")\n",
        "h2o.cluster().shutdown()\n",
        "log_mensagem(\"Cluster H2O desligado.\")"
      ],
      "metadata": {
        "id": "My8kS-CuniHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Etapa 13: Modelagem Final com H2O AutoML, CV e Features Híbridas"
      ],
      "metadata": {
        "id": "bgcERhVB8C2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.1. Introdução\n",
        "\n",
        "Esta é a etapa culminante de nossa experimentação. Tendo estabelecido que a Validação Cruzada (CV) é a metodologia de treinamento mais robusta (Etapa 12) e que as features híbridas (`Latitude`/`Longitude` + `Cluster`) apresentam o maior potencial preditivo (Etapa 9.3), esta etapa une as duas abordagens.\n",
        "\n",
        "O objetivo é treinar os modelos de H2O AutoML utilizando a Validação Cruzada no nosso dataset mais rico, que contém tanto a localização granular quanto a informação de zona. A hipótese é que esta combinação resultará nos modelos de mais alta performance de todo o projeto, representando o estado da arte para este problema com os dados disponíveis.\n",
        "\n",
        "## 13.2. Objetivos\n",
        "\n",
        "1.  **Combinar as Melhores Técnicas:** Unir a metodologia de treinamento com Validação Cruzada do H2O com o dataset que inclui a feature `localizacao_cluster`.\n",
        "2.  **Treinar os Modelos Finais:** Executar a pipeline de modelagem final para os segmentos vertical e horizontal.\n",
        "3.  **Obter a Performance Definitiva:** Avaliar os modelos resultantes no conjunto de teste para obter as métricas finais de performance do projeto."
      ],
      "metadata": {
        "id": "WIm91tlo8EEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "!pip install h2o -q\n",
        "import h2o\n",
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "DRIVE_BASE_PATH = '/content/drive/MyDrive/dados_tcc'\n",
        "\n",
        "STAGE_09_1_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_09_1_kmeans_clustering')\n",
        "CLUSTERED_DATASET_PATH = os.path.join(STAGE_09_1_OUTPUT_DIR, 'dataset_com_clusters.parquet')\n",
        "\n",
        "STAGE_13_OUTPUT_DIR = os.path.join(DRIVE_BASE_PATH, 'stage_13_h2o_cv_hybrid')\n",
        "LOG_FILE_PATH = os.path.join(STAGE_13_OUTPUT_DIR, 'log_etapa_13.txt')\n",
        "os.makedirs(STAGE_13_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(LOG_FILE_PATH, 'w') as f:\n",
        "    f.write(f\"--- LOG DE MODELAGEM FINAL (H2O + CV + HÍBRIDO) DA ETAPA 13 ---\\n\")\n",
        "    f.write(f\"Execução iniciada em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "print(f\"Diretório de saída da Etapa 13 criado em: {STAGE_13_OUTPUT_DIR}\")\n",
        "h2o.init()"
      ],
      "metadata": {
        "id": "yYGCr0PO8Dkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.3. Funções de Preparação e Modelagem\n",
        "\n",
        "Reutilizamos as funções de preparação de dados e adaptamos a função de modelagem do AutoML para garantir que ela utilize a Validação Cruzada e trate a feature `localizacao_cluster` como um fator categórico."
      ],
      "metadata": {
        "id": "xYpPlvGL8ITu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_mensagem(mensagem, tipo='INFO'):\n",
        "    \"\"\"Registra uma mensagem formatada no console e no arquivo de log.\"\"\"\n",
        "    log_formatado = f\"[{tipo}] {datetime.now().strftime('%H:%M:%S')} - {mensagem}\"\n",
        "    print(log_formatado)\n",
        "    with open(LOG_FILE_PATH, 'a', encoding='utf-8') as f:\n",
        "        f.write(log_formatado + \"\\n\")\n",
        "\n",
        "def preparar_dataset_para_modelagem(df):\n",
        "    log_mensagem(\"Iniciando preparação completa do dataset clusterizado...\")\n",
        "    colunas_numericas = ['valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida','area_terreno', 'testada', 'fracao_ideal', 'area_construida']\n",
        "    for col in colunas_numericas:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df['valor_venal_referencia_proporcional'] = df['valor_venal_referencia'] * (df['proporcao_transmitida'] / 100)\n",
        "    df['base_calculo'] = df[['valor_transacao', 'valor_venal_referencia_proporcional']].max(axis=1)\n",
        "    inflacao_anual = {2006: 3.14, 2007: 4.46, 2008: 5.90, 2009: 4.31, 2010: 5.91, 2011: 6.50, 2012: 5.84, 2013: 5.91, 2014: 6.41, 2015: 10.67, 2016: 6.29, 2017: 2.95, 2018: 3.75, 2019: 4.31, 2020: 4.52, 2021: 10.06, 2022: 5.79, 2023: 4.62, 2024: 4.83}\n",
        "    fatores_anual = {ano: 1 + (taxa / 100) for ano, taxa in inflacao_anual.items()}\n",
        "    fatores_correcao = {}\n",
        "    anos_ordenados = sorted(fatores_anual.keys(), reverse=True)\n",
        "    fator_acumulado = 1.0\n",
        "    for ano in anos_ordenados:\n",
        "        fatores_correcao[ano] = fator_acumulado\n",
        "        fator_acumulado *= fatores_anual[ano]\n",
        "    df['fator_correcao'] = df['ANO'].map(fatores_correcao)\n",
        "    df['base_calculo_corrigida_2024'] = (df['base_calculo'] * df['fator_correcao']).round(2)\n",
        "    limite_inferior = df['base_calculo_corrigida_2024'].quantile(0.05)\n",
        "    limite_superior = df['base_calculo_corrigida_2024'].quantile(0.95)\n",
        "    df = df[(df['base_calculo_corrigida_2024'] >= limite_inferior) & (df['base_calculo_corrigida_2024'] <= limite_superior)].copy()\n",
        "    colunas_para_remover = [\n",
        "        'n_cadastro', 'matricula_imovel', 'cep', 'natureza_transacao', 'data_transacao', 'MES_ANO',\n",
        "        'valor_financiado', 'tipo_financiamento', 'cartorio_de_registro', 'situacao_no_sql',\n",
        "        'uso_iptu', 'padrao_iptu', 'descricao_uso_iptu', 'Endereço', 'Bairro', 'Cidade', 'Estado', 'Endereço completo',\n",
        "        'valor_transacao', 'valor_venal_referencia', 'proporcao_transmitida',\n",
        "        'valor_venal_referencia_proporcional', 'base_calculo_original', 'base_calculo', 'fator_correcao'\n",
        "    ]\n",
        "    df_final = df.drop(columns=[col for col in colunas_para_remover if col in df.columns])\n",
        "    log_mensagem(\"Preparação completa do dataset concluída. Shape final: \" + str(df_final.shape))\n",
        "    return df_final\n",
        "\n",
        "def treinar_com_automl_cv_hibrido(df, nome_modelo):\n",
        "    \"\"\"\n",
        "    Executa a pipeline final com H2O AutoML, CV e features híbridas.\n",
        "    \"\"\"\n",
        "    log_mensagem(f\"--- INICIANDO TREINAMENTO FINAL (H2O+CV+Híbrido) PARA O MODELO: {nome_modelo} ---\")\n",
        "\n",
        "    TARGET = 'base_calculo_corrigida_2024'\n",
        "    features_cols = [col for col in df.columns if col != TARGET]\n",
        "\n",
        "    hf = h2o.H2OFrame(df)\n",
        "    hf['localizacao_cluster'] = hf['localizacao_cluster'].asfactor()\n",
        "    log_mensagem(\"DataFrame convertido para H2OFrame e 'localizacao_cluster' definido como fator.\")\n",
        "\n",
        "    train_full = hf[hf['ANO'] <= 2020, :]\n",
        "    test = hf[hf['ANO'] >= 2021, :]\n",
        "    log_mensagem(f\"Tamanhos: Treino (para CV)={train_full.shape[0]}, Teste={test.shape[0]}\")\n",
        "\n",
        "    x = [col for col in features_cols if col != 'ANO']\n",
        "    y = TARGET\n",
        "\n",
        "    log_mensagem(\"Iniciando H2O AutoML com 5-fold Cross-Validation... (max_runtime_secs=600)\")\n",
        "    aml = H2OAutoML(max_runtime_secs=600, nfolds=5, seed=42, sort_metric=\"RMSE\")\n",
        "    aml.train(x=x, y=y, training_frame=train_full, leaderboard_frame=test)\n",
        "\n",
        "    lb = aml.leaderboard\n",
        "    log_mensagem(f\"\\n--- Leaderboard (CV) para o Modelo {nome_modelo} ---\")\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        log_mensagem(lb.as_data_frame().to_string())\n",
        "\n",
        "    leader_model = aml.leader\n",
        "    log_mensagem(f\"\\nMelhor modelo encontrado (com base na CV): {leader_model.model_id}\")\n",
        "\n",
        "    perf_test = leader_model.model_performance(test_data=test)\n",
        "    log_mensagem(f\"\\n--- Performance Final do Melhor Modelo no Conjunto de Teste ({nome_modelo}) ---\")\n",
        "    log_mensagem(str(perf_test))\n",
        "\n",
        "    log_mensagem(f\"--- FINALIZADO TREINAMENTO FINAL PARA O MODELO: {nome_modelo} ---\\n\")"
      ],
      "metadata": {
        "id": "OtOwKN3l8I49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_mensagem(\"--- INICIANDO PROCESSAMENTO FINAL (H2O + CV + HÍBRIDO - ETAPA 13) ---\")\n",
        "\n",
        "try:\n",
        "    df_clusterizado = pd.read_parquet(CLUSTERED_DATASET_PATH)\n",
        "    log_mensagem(f\"Dataset clusterizado da Etapa 9.1 carregado com {len(df_clusterizado)} registros.\")\n",
        "\n",
        "    df_modelagem = preparar_dataset_para_modelagem(df_clusterizado)\n",
        "\n",
        "    df_modelagem['descricao_padrao_iptu'] = df_modelagem['descricao_padrao_iptu'].str.strip()\n",
        "    df_vertical = df_modelagem[df_modelagem['descricao_padrao_iptu'] == 'RESIDENCIAL VERTICAL'].drop(columns=['descricao_padrao_iptu', 'area_terreno', 'testada'])\n",
        "    df_horizontal = df_modelagem[df_modelagem['descricao_padrao_iptu'] == 'RESIDENCIAL HORIZONTAL'].drop(columns=['descricao_padrao_iptu', 'fracao_ideal'])\n",
        "\n",
        "    treinar_com_automl_cv_hibrido(df_vertical, \"Vertical\")\n",
        "    treinar_com_automl_cv_hibrido(df_horizontal, \"Horizontal\")\n",
        "\n",
        "except Exception as e:\n",
        "    log_mensagem(f\"Ocorreu um erro inesperado na Etapa 13: {e}\", tipo='ERRO CRÍTICO')\n",
        "\n",
        "log_mensagem(\"--- FINALIZAÇÃO DA PIPELINE DE MODELAGEM FINAL (ETAPA 13) ---\", tipo=\"SUMÁRIO\")\n",
        "h2o.cluster().shutdown()\n",
        "log_mensagem(\"Cluster H2O desligado.\")"
      ],
      "metadata": {
        "id": "FYMukYn68NDh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}